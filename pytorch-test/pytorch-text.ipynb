{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ab72ca",
   "metadata": {},
   "source": [
    "# seq to seq and attention\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b35d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31758ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1717d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ba16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # if not SOS or EOS, then the index \n",
    "            # will start from 2\n",
    "            self.word2index[word] = self.n_words\n",
    "            # The first actual word with counting\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde193a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    lines = open(\"./data/english-lang-trans/%s-%s.txt\" % (lang1, lang2),\\\n",
    "                encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    \n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    return input_lang, output_lang, pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the data set to only relatively short and simple sentences.\n",
    "\n",
    "MAX_LENGTH=10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc289b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s senence pairs\" % len(pairs))\n",
    "    print(\"Counting words\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    print(\"Counted words: \")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b7770",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\", True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8dee4",
   "metadata": {},
   "source": [
    "## seq2seq working flow\n",
    "![seq2seq working flow](https://pytorch.org/tutorials/_images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d13148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1b1f5d",
   "metadata": {},
   "source": [
    "![seq2seq encoding - decoding](https://pytorch.org/tutorials/_images/decoder-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef977c3",
   "metadata": {},
   "source": [
    "**Understanding squeeze(-1) and Detach in Decoder Input Processing (Seq2seq with Teacher Forcing):**\n",
    "\n",
    "In a seq2seq model with teacher forcing, `squeeze(-1)` and `detach` are used together during decoder input processing to ensure proper shape compatibility and memory optimization. Here's a detailed breakdown:\n",
    "\n",
    "**1. Context and Teacher Forcing:**\n",
    "\n",
    "- Seq2seq models translate between sequences (e.g., text-to-text, speech-to-text).\n",
    "- Teacher forcing is a training technique where the model receives the correct target sequence as input at each decoding step, along with previously generated elements.\n",
    "\n",
    "**2. Decoder Input Processing:**\n",
    "\n",
    "   **a. Target Sequence Reshaping (unsqueeze(1))**\n",
    "\n",
    "     - The target sequence (ground truth) typically has a shape `(batch_size, target_length)`.\n",
    "     - For teacher forcing, the decoder needs input one element at a time during each decoding step.\n",
    "     - To achieve this, `unsqueeze(1)` is used during training:\n",
    "\n",
    "       ```python\n",
    "       # Example target sequence\n",
    "       target_sequence = torch.randint(1, 10, (batch_size, target_length))\n",
    "\n",
    "       # Teacher forcing input for first decoding step\n",
    "       teacher_forcing_input = target_sequence[:, 0].unsqueeze(1)\n",
    "       ```\n",
    "\n",
    "     - Explanation:\n",
    "       - `target_sequence[:, 0]` extracts the first element (index 0) from each sequence in the batch, resulting in a tensor of shape `(batch_size,)`.\n",
    "       - `unsqueeze(1)` inserts a new dimension of size 1 at dimension 1 (the column dimension). This creates a tensor with shape `(batch_size, 1)`, aligning with the expected decoder input format at the first step.\n",
    "\n",
    "   **b. Decoder Output Reshaping and Detachment (squeeze(-1).detach())**\n",
    "\n",
    "     - After each decoding step, the decoder generates an output.\n",
    "     - We need to compare the decoder output with the corresponding element in the target sequence for calculating the loss during training.\n",
    "     - However, the decoder output might have a shape `(batch_size, target_length, hidden_size)`:\n",
    "\n",
    "       - `batch_size`: Number of samples in the batch.\n",
    "       - `target_length`: Length of the target sequence.\n",
    "       - `hidden_size`: Model's internal hidden state dimension (representing extracted features).\n",
    "\n",
    "     ```python\n",
    "     # Example decoder output after a decoding step\n",
    "     decoder_output = model(decoder_input)  # Model processes decoder input\n",
    "\n",
    "     # Process decoder output for loss calculation\n",
    "     processed_decoder_output = decoder_output.squeeze(-1).detach()\n",
    "     ```\n",
    "\n",
    "     - Explanation:\n",
    "       - `squeeze(-1)` removes the dimension of size 1 at the last dimension (dimension -1). This ensures the decoder output has the same shape `(batch_size, target_length)` as the target sequence, allowing for element-wise comparison during loss calculation. Essentially, it extracts the meaningful content from the last dimension (`hidden_size`) by combining it with the previous dimensions (`batch_size` and `target_length`).\n",
    "       - `.detach()` detaches the processed decoder output from the computational graph. Since we're using the ground truth for teacher forcing and not backpropagating through the decoder output in this step, detaching saves memory and avoids unnecessary computations.\n",
    "\n",
    "**3. Key Points:**\n",
    "\n",
    "- `unsqueeze(1)` prepares the target sequence for teacher forcing by creating an input with the correct shape for the first decoding step.\n",
    "- `squeeze(-1).detach()` processes the decoder output by:\n",
    "   - Removing the unnecessary `hidden_size` dimension for element-wise comparison.\n",
    "   - Detaching the output from the computational graph for teacher forcing efficiency.\n",
    "\n",
    "**In essence, these operations ensure that the decoder receives the appropriate teacher forcing input and that the decoder output is properly shaped for loss calculation during training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long,\\\n",
    "                                   device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input,\n",
    "                                                               decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            \n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing, feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # teacher forcing\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach() # detach from hist as input\n",
    "                \n",
    "        decoder_outputs = torch.cat(decoer_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        # return `None` for consistency in the training loop\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "    \n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad31d4",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/1152PYf.png)  \n",
    "![](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing taining data\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\", True)\n",
    "    \n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    \n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "        \n",
    "        \n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, \n",
    "                                  batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afb64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to print time elapsed and estimated time remaining given the current time and progress %.\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a7f1f",
   "metadata": {},
   "source": [
    "> Change the backend to TkAgg, QtAgg, or WXAgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('QtAgg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual train, evaluation\n",
    "\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
    "\n",
    "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
    "\n",
    "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
    "\n",
    "evaluateAndShowAttention('je suis reellement fiere de vous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a094df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aead68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ced821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fd56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504268e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03422c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f3970f",
   "metadata": {},
   "source": [
    "# preprocess custom text dataset using torchtext \n",
    "https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html\n",
    "\n",
    "Read a dataset\n",
    "\n",
    "Tokenize sentence\n",
    "\n",
    "Apply transforms to sentence\n",
    "\n",
    "Perform bucket batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "zh = spacy.load(\"zh_core_web_sm\")\n",
    "de = spacy.load(\"de_core_news_sm\")\n",
    "fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"data/deu-eng/deu.txt\"\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode=\"rb\")\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter=\"\\t\", as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSample(data_pipe):\n",
    "    for sample in data_pipe:\n",
    "        print(sample)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "showSample(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"\n",
    "    Keeping the first two elements in a tuple\n",
    "    Keeping the first and second columns\n",
    "    \"\"\"\n",
    "    return row[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8112316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "showSample(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ac89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize a German text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for the tokenization\n",
    "print(engTokenize(\"Hello world!!!\"))\n",
    "print(deTokenize(\"Hallo Welt!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(engTokenize(\"Have a good day!!!\"))\n",
    "print(deTokenize(\"Haben Sie einen guten Tag!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795684cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since, our iterator contains\n",
    "    tuple of sentences (source and target), `place` parameters defines for which\n",
    "    index to return the tokens for. `place=0` for source and `place=1` for target\n",
    "    \"\"\"\n",
    "    for english, german in data_iter:\n",
    "        if place == 0:\n",
    "            yield engTokenize(english)\n",
    "        else:\n",
    "            yield deTokenize(german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe, 0),\n",
    "    min_freq=2,\n",
    "    specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
    "    special_first=True\n",
    ")\n",
    "source_vocab.set_default_index(source_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,1),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for vocab\n",
    "print(source_vocab.get_itos()[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalize sentences using vocabulary\n",
    "# convert our sentences to corresponding indices\n",
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_transform = T.Sequential(\n",
    "        # converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997f78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5321c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[798][0]\n",
    "print(f\"Some sentence = \", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence = \", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "    \n",
    "    return (\n",
    "        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n",
    "        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(applyTransform)\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_string = source_vocab.get_itos()\n",
    "for inx_list in temp_list[0]:\n",
    "#     print(index_to_string[index], end=\"\")\n",
    "    output = []\n",
    "    \n",
    "    for inx in inx_list:\n",
    "        print(index_to_string[inx], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 4,\n",
    "    batch_num = 5,\n",
    "    bucket_num = 1,\n",
    "    use_in_batch_shuffle=False,\n",
    "    sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(data_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f351594",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the function to each element in the iterator\n",
    "data_pipe = data_pipe.map(separateSourceTarget)\n",
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55649084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply paddjing\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd304d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  index to string mapping to see how the sequence would look with tokens instead of indices\n",
    "\n",
    "source_index_to_string = source_vocab.get_itos()\n",
    "target_index_to_string = target_vocab.get_itos()\n",
    "\n",
    "\n",
    "def showSomeTransformedSentences(data_pipe):\n",
    "    \"\"\"\n",
    "    Function to show how the sentences look like after applying all transforms.\n",
    "    Here we try to print actual words instead of corresponding index\n",
    "    \"\"\"\n",
    "    for sources,targets in data_pipe:\n",
    "        if sources[0][-1] != 0:\n",
    "            continue # Just to visualize padding of shorter sentences\n",
    "        for i in range(4):\n",
    "            source = \"\"\n",
    "            for token in sources[i]:\n",
    "                source += \" \" + source_index_to_string[token]\n",
    "            target = \"\"\n",
    "            for token in targets[i]:\n",
    "                target += \" \" + target_index_to_string[token]\n",
    "            print(f\"Source: {source}\")\n",
    "            print(f\"Target: {target}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "showSomeTransformedSentences(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f2fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce04c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f51d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32e471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6ff75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "418a1c54",
   "metadata": {},
   "source": [
    "# Text classification with the torchtext library\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "`!pip install -U portalocker>=2.0.0``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe588ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3faa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(AG_NEWS(split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82046ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n",
      "(3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')\n",
      "(3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")\n",
      "(3, 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.')\n",
      "(3, 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.')\n",
      "(3, 'Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\\\but stayed near lows for the year as oil prices surged past  #36;46\\\\a barrel, offsetting a positive outlook from computer maker\\\\Dell Inc. (DELL.O)')\n",
      "(3, \"Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\")\n",
      "(3, 'Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.')\n",
      "(3, 'Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.')\n",
      "(3, \"Wall St. Bears Claw Back Into the Black  NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.\")\n",
      "(3, \"Oil and Economy Cloud Stocks' Outlook  NEW YORK (Reuters) - Soaring crude prices plus worries  about the economy and the outlook for earnings are expected to  hang over the stock market next week during the depth of the  summer doldrums.\")\n",
      "(3, \"No Need for OPEC to Pump More-Iran Gov  TEHRAN (Reuters) - OPEC can do nothing to douse scorching  oil prices when markets are already oversupplied by 2.8 million  barrels per day (bpd) of crude, Iran's OPEC governor said  Saturday, warning that prices could fall sharply.\")\n",
      "(3, 'Non-OPEC Nations Should Up Output-Purnomo  JAKARTA (Reuters) - Non-OPEC oil exporters should consider  increasing output to cool record crude prices, OPEC President  Purnomo Yusgiantoro said on Sunday.')\n",
      "(3, \"Google IPO Auction Off to Rocky Start  WASHINGTON/NEW YORK (Reuters) - The auction for Google  Inc.'s highly anticipated initial public offering got off to a  rocky start on Friday after the Web search company sidestepped  a bullet from U.S. securities regulators.\")\n",
      "(3, \"Dollar Falls Broadly on Record Trade Gap  NEW YORK (Reuters) - The dollar tumbled broadly on Friday  after data showing a record U.S. trade deficit in June cast  fresh doubts on the economy's recovery and its ability to draw  foreign capital to fund the growing gap.\")\n",
      "(3, \"Rescuing an Old Saver If you think you may need to help your elderly relatives with their finances, don't be shy about having the money talk -- soon.\")\n",
      "(3, 'Kids Rule for Back-to-School The purchasing power of kids is a big part of why the back-to-school season has become such a huge marketing phenomenon.')\n",
      "(3, \"In a Down Market, Head Toward Value Funds There is little cause for celebration in the stock market these days, but investors in value-focused mutual funds have reason to feel a bit smug -- if only because they've lost less than the folks who stuck with growth.\")\n",
      "(3, 'US trade deficit swells in June The US trade deficit has exploded 19 to a record \\\\$55.8bn as oil costs drove imports higher, according to a latest figures.')\n",
      "(3, \"Shell 'could be target for Total' Oil giant Shell could be bracing itself for a takeover attempt, possibly from French rival Total, a  press report claims.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(next(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79290ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f10d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5071a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb4528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be0bbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 30, 5297]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97f35f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12544, 9199, 6, 269]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([\"hello\", \"gears\", \"of\", \"war\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb944056",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52ea93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 2, 30, 5297]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72a5cad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe037801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72eaf109",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7fbd2",
   "metadata": {},
   "source": [
    "### why offsets\n",
    "\n",
    "Certainly! Here's a concrete example with more than 5 sequences to illustrate offset computation with `cumsum` and slicing:\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "Imagine a batch containing six text sequences of varying lengths:\n",
    "\n",
    "- Sequence 1: \"This is the first sentence\" (length 5)\n",
    "- Sequence 2: \"A shorter sentence\" (length 4)\n",
    "- Sequence 3: \"Another example sentence\" (length 5)\n",
    "- Sequence 4: \"This is sequence number four\" (length 7)\n",
    "- Sequence 5: \"A very brief message\" (length 3)\n",
    "- Sequence 6: \"The final sequence of the batch\" (length 6)\n",
    "\n",
    "**`collate_batch` Function:**\n",
    "\n",
    "1. **Processing Sequences:**\n",
    "\n",
    "   - Processes each sequence using `text_pipeline` (e.g., tokenization), resulting in processed tensors.\n",
    "\n",
    "2. **Creating `offsets`:**\n",
    "\n",
    "   - Keeps track of starting indices by appending the length of each processed sequence:\n",
    "\n",
    "     ```\n",
    "     offsets = [0, 5, 9, 14, 21, 24]\n",
    "     ```\n",
    "\n",
    "     - `offsets[0]` to `offsets[5]` represent the accumulated lengths of sequences 1 to 6, respectively.\n",
    "\n",
    "**Offsets Processing:**\n",
    "\n",
    "1. **Slicing (Optional, depending on implementation):**\n",
    "\n",
    "   - `offsets[:-1] = [0, 5, 9, 14, 21]`: This excludes the last element (total length) if it's present.\n",
    "\n",
    "2. **Converting to Tensor:**\n",
    "\n",
    "   - `torch.tensor(offsets[:-1])`: Converts the sliced `offsets` (or the entire list if not sliced) into a PyTorch tensor.\n",
    "\n",
    "3. **Cumulative Sum:**\n",
    "\n",
    "   - ```\n",
    "     .cumsum(dim=0)\n",
    "     ```\n",
    "\n",
    "     :\n",
    "\n",
    "      Calculates the starting indices for each sequence:\n",
    "\n",
    "     ```\n",
    "     offsets_with_indices = torch.tensor([0, 5, 9, 14, 21, 27]).cumsum(dim=0)\n",
    "     ```\n",
    "\n",
    "     - `offsets_with_indices` becomes `tensor([0, 5, 9, 14, 21, 27])`.\n",
    "\n",
    "**Understanding the Results:**\n",
    "\n",
    "- ```\n",
    "  offsets_with_indices\n",
    "  ```\n",
    "\n",
    "   now holds the absolute starting indices for each sequence within the combined \n",
    "\n",
    "  ```\n",
    "  text_list\n",
    "  ```\n",
    "\n",
    "   tensor:\n",
    "\n",
    "  - Sequence 1: Starts at index `offsets_with_indices[0]` (0).\n",
    "  - Sequence 2: Starts at index `offsets_with_indices[1]` (5).\n",
    "  - Sequence 3: Starts at index `offsets_with_indices[2]` (9).\n",
    "  - And so on...\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Slicing with `offsets[:-1]` addresses the potential issue of including the total length in `offsets`.\n",
    "- The cumulative sum (`cumsum`) provides the correct starting indices for efficient processing of each sequence in the batch.\n",
    "\n",
    "I hope this example with six sequences clarifies the concept of offsets, slicing, and `cumsum` in the context of handling variable-length text data during batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d458835",
   "metadata": {},
   "source": [
    "<u>The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9156e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [],[],[0]\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        \n",
    "        label_list.append(label_pipeline(_label))\n",
    "        \n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "        offsets.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b0dd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # about cumsum function\n",
    "\n",
    "# import torch\n",
    "\n",
    "# # Sample tensor\n",
    "# tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# # Cumulative sum across rows (dim=0)\n",
    "# cumulative_sum_rows = torch.cumsum(tensor, dim=0)\n",
    "# print(cumulative_sum_rows)  # Output: tensor([[1, 3, 6], [4, 9, 15]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a2142b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8,\n",
    "    shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f4d44",
   "metadata": {},
   "source": [
    "The `offsets` parameter plays a crucial role in efficiently handling variable-length text sequences within the `embedding` layer (specifically, the `nn.EmbeddingBag` module). Here's a breakdown of how it works:\n",
    "\n",
    "**`nn.EmbeddingBag` for Variable-Length Sequences:**\n",
    "\n",
    "- The standard `nn.Embedding` module assumes all sequences have the same length.\n",
    "- However, in text classification, sequences often have different lengths.\n",
    "- `nn.EmbeddingBag` overcomes this limitation by efficiently processing variable-length sequences.\n",
    "\n",
    "**How Offsets Help:**\n",
    "\n",
    "1. **Combined Text List (`text`):**\n",
    "\n",
    "   - Imagine a tensor `text` that combines all processed sequences (e.g., tokenized) from the batch into a single list.\n",
    "   - This `text` tensor might contain all the tokens from all sequences, one after another.\n",
    "\n",
    "2. **`offsets`:**\n",
    "\n",
    "   - This parameter is a tensor (often created from a list) that provides the starting index for each sequence within the combined `text` tensor.\n",
    "\n",
    "   - For example,\n",
    "\n",
    "      \n",
    "\n",
    "     ```\n",
    "     offsets = [0, 5, 9]\n",
    "     ```\n",
    "\n",
    "      might indicate that:\n",
    "\n",
    "     - Sequence 1 starts at index 0 in `text`.\n",
    "     - Sequence 2 starts at index 5 in `text`.\n",
    "     - Sequence 3 starts at index 9 in `text`.\n",
    "\n",
    "**`embedding(text, offsets)` in Action:**\n",
    "\n",
    "1. Leveraging Offsets:\n",
    "   - The `embedding` layer (using `nn.EmbeddingBag`) uses `offsets` to identify the relevant subsequence for each sequence within the combined `text` tensor.\n",
    "2. Embedding Calculation:\n",
    "   - For each sequence, it extracts the corresponding subsequence from `text` based on the starting index provided by `offsets`.\n",
    "   - This subsequence represents the sequence's tokens.\n",
    "3. Embedding Bag Operation:\n",
    "   - `nn.EmbeddingBag` then performs embedding on the extracted subsequence, essentially converting each token in the sequence to a dense vector representation.\n",
    "\n",
    "**Benefits of `offsets`:**\n",
    "\n",
    "- Enables efficient processing of variable-length sequences in a single batch.\n",
    "- Reduces memory overhead compared to padding sequences to a fixed length.\n",
    "\n",
    "**Further Explanation:**\n",
    "\n",
    "- The `nn.Embedding` layer likely has a pre-trained weight matrix that maps each word in the vocabulary to a dense embedding vector (e.g., size `embed_dim`).\n",
    "- By looking up the indices of tokens in the subsequence (extracted using `offsets`), the `embedding` layer creates an embedding representation for the entire sequence.\n",
    "\n",
    "**In essence, `offsets` act as a guide for the `embedding` layer to efficiently extract and process individual sequences within the combined `text` tensor, enabling effective text classification even with variable-length text data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59a58902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange=.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864731ce",
   "metadata": {},
   "source": [
    "## The AG_NEWS dataset has four labels and therefore the number of classes is four.\n",
    "1 : World   \n",
    "2 : Sports    \n",
    "3 : Business    \n",
    "4 : Sci/Tec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d5cec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split=\"train\")\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3005718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d7c3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0,0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d} | {:5d} batches\"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            total_acc, total_count = 0,0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2318e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead8f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e8eba4a",
   "metadata": {},
   "source": [
    "# random_split and to_map_style_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb039ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "9\n",
      "2\n",
      "4\n",
      "---\n",
      "1\n",
      "0\n",
      "7\n",
      "---\n",
      "6\n",
      "5\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(40)\n",
    "a = random_split(range(10), [0.5, 0.3, 0.2], generator=generator)\n",
    "for group in a:\n",
    "    for each in group:\n",
    "        print(each)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a173bcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "9\n",
      "---\n",
      "2\n",
      "4\n",
      "1\n",
      "0\n",
      "7\n",
      "6\n",
      "5\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(40)\n",
    "a = random_split(range(10), [3,7], generator=generator)\n",
    "for group in a:\n",
    "    for each in group:\n",
    "        print(each)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_datset, [num_train, lean(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6004949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.904\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53ba4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn.  Four days ago, Jon Rahm was \\\n",
    "    enduring the seasons worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursdays first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering hed never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161e1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2885972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcb0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27f73c9",
   "metadata": {},
   "source": [
    "# Language Translation with nn.Transformer and torchtext \\\n",
    "https://www.statmt.org/wmt16/multimodal-task.html#task1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebe609",
   "metadata": {},
   "source": [
    "### trochtext.datasets\n",
    "\n",
    "Datasets\n",
    "\n",
    "- [Text Classification](https://pytorch.org/text/stable/datasets.html#text-classification)\n",
    "  - [AG_NEWS](https://pytorch.org/text/stable/datasets.html#ag-news)\n",
    "  - [AmazonReviewFull](https://pytorch.org/text/stable/datasets.html#amazonreviewfull)\n",
    "  - [AmazonReviewPolarity](https://pytorch.org/text/stable/datasets.html#amazonreviewpolarity)\n",
    "  - [CoLA](https://pytorch.org/text/stable/datasets.html#cola)\n",
    "  - [DBpedia](https://pytorch.org/text/stable/datasets.html#dbpedia)\n",
    "  - [IMDb](https://pytorch.org/text/stable/datasets.html#imdb)\n",
    "  - [MNLI](https://pytorch.org/text/stable/datasets.html#mnli)\n",
    "  - [MRPC](https://pytorch.org/text/stable/datasets.html#mrpc)\n",
    "  - [QNLI](https://pytorch.org/text/stable/datasets.html#qnli)\n",
    "  - [QQP](https://pytorch.org/text/stable/datasets.html#qqp)\n",
    "  - [RTE](https://pytorch.org/text/stable/datasets.html#rte)\n",
    "  - [SogouNews](https://pytorch.org/text/stable/datasets.html#sogounews)\n",
    "  - [SST2](https://pytorch.org/text/stable/datasets.html#sst2)\n",
    "  - [STSB](https://pytorch.org/text/stable/datasets.html#stsb)\n",
    "  - [WNLI](https://pytorch.org/text/stable/datasets.html#wnli)\n",
    "  - [YahooAnswers](https://pytorch.org/text/stable/datasets.html#yahooanswers)\n",
    "  - [YelpReviewFull](https://pytorch.org/text/stable/datasets.html#yelpreviewfull)\n",
    "  - [YelpReviewPolarity](https://pytorch.org/text/stable/datasets.html#yelpreviewpolarity)\n",
    "- [Language Modeling](https://pytorch.org/text/stable/datasets.html#language-modeling)\n",
    "  - [PennTreebank](https://pytorch.org/text/stable/datasets.html#penntreebank)\n",
    "  - [WikiText-2](https://pytorch.org/text/stable/datasets.html#wikitext-2)\n",
    "  - [WikiText103](https://pytorch.org/text/stable/datasets.html#wikitext103)\n",
    "- [Machine Translation](https://pytorch.org/text/stable/datasets.html#machine-translation)\n",
    "  - [IWSLT2016](https://pytorch.org/text/stable/datasets.html#iwslt2016)\n",
    "  - [IWSLT2017](https://pytorch.org/text/stable/datasets.html#iwslt2017)\n",
    "  - [Multi30k](https://pytorch.org/text/stable/datasets.html#multi30k)\n",
    "- [Sequence Tagging](https://pytorch.org/text/stable/datasets.html#sequence-tagging)\n",
    "  - [CoNLL2000Chunking](https://pytorch.org/text/stable/datasets.html#conll2000chunking)\n",
    "  - [UDPOS](https://pytorch.org/text/stable/datasets.html#udpos)\n",
    "- [Question Answer](https://pytorch.org/text/stable/datasets.html#question-answer)\n",
    "  - [SQuAD 1.0](https://pytorch.org/text/stable/datasets.html#squad-1-0)\n",
    "  - [SQuAD 2.0](https://pytorch.org/text/stable/datasets.html#squad-2-0)\n",
    "- [Unsupervised Learning](https://pytorch.org/text/stable/datasets.html#unsupervised-learning)\n",
    "  - [CC100](https://pytorch.org/text/stable/datasets.html#cc100)\n",
    "  - [EnWik9](https://pytorch.org/text/stable/datasets.html#enwik9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6857b39",
   "metadata": {},
   "source": [
    "Python offers a typing module that provides various data type annotations to improve code readability and maintainability.\n",
    "These annotations don't affect the code's functionality at runtime, but they act as hints for developers and static type checkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e715a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# typing, Iterable, and List\n",
    "from typing import Iterable, List\n",
    "\n",
    "def print_all_items(items: Iterable) -> None:\n",
    "    \"\"\"Prints all items in an iterable object.\"\"\"\n",
    "    for item in items:\n",
    "        print(item)\n",
    "\n",
    "# Example usage\n",
    "my_list: List[int] = [1, 2, 3]\n",
    "print_all_items(my_list)  # This works as the list is iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52663d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sourcing and processing\n",
    "\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "478c407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9010fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = \"de\"\n",
    "TGT_LANGUAGE = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3841b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7a3a0",
   "metadata": {},
   "source": [
    "```shell\n",
    "pip install -U torchdata\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a82a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source and target language tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98228e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
