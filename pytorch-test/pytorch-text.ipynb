{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ab72ca",
   "metadata": {},
   "source": [
    "# seq to seq and attention\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b35d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31758ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1717d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ba16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # if not SOS or EOS, then the index \n",
    "            # will start from 2\n",
    "            self.word2index[word] = self.n_words\n",
    "            # The first actual word with counting\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde193a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    lines = open(\"./data/english-lang-trans/%s-%s.txt\" % (lang1, lang2),\\\n",
    "                encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    \n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    return input_lang, output_lang, pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the data set to only relatively short and simple sentences.\n",
    "\n",
    "MAX_LENGTH=10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc289b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s senence pairs\" % len(pairs))\n",
    "    print(\"Counting words\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    print(\"Counted words: \")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b7770",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\", True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8dee4",
   "metadata": {},
   "source": [
    "## seq2seq working flow\n",
    "![seq2seq working flow](https://pytorch.org/tutorials/_images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d13148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1b1f5d",
   "metadata": {},
   "source": [
    "![seq2seq encoding - decoding](https://pytorch.org/tutorials/_images/decoder-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef977c3",
   "metadata": {},
   "source": [
    "**Understanding squeeze(-1) and Detach in Decoder Input Processing (Seq2seq with Teacher Forcing):**\n",
    "\n",
    "In a seq2seq model with teacher forcing, `squeeze(-1)` and `detach` are used together during decoder input processing to ensure proper shape compatibility and memory optimization. Here's a detailed breakdown:\n",
    "\n",
    "**1. Context and Teacher Forcing:**\n",
    "\n",
    "- Seq2seq models translate between sequences (e.g., text-to-text, speech-to-text).\n",
    "- Teacher forcing is a training technique where the model receives the correct target sequence as input at each decoding step, along with previously generated elements.\n",
    "\n",
    "**2. Decoder Input Processing:**\n",
    "\n",
    "   **a. Target Sequence Reshaping (unsqueeze(1))**\n",
    "\n",
    "     - The target sequence (ground truth) typically has a shape `(batch_size, target_length)`.\n",
    "     - For teacher forcing, the decoder needs input one element at a time during each decoding step.\n",
    "     - To achieve this, `unsqueeze(1)` is used during training:\n",
    "\n",
    "       ```python\n",
    "       # Example target sequence\n",
    "       target_sequence = torch.randint(1, 10, (batch_size, target_length))\n",
    "\n",
    "       # Teacher forcing input for first decoding step\n",
    "       teacher_forcing_input = target_sequence[:, 0].unsqueeze(1)\n",
    "       ```\n",
    "\n",
    "     - Explanation:\n",
    "       - `target_sequence[:, 0]` extracts the first element (index 0) from each sequence in the batch, resulting in a tensor of shape `(batch_size,)`.\n",
    "       - `unsqueeze(1)` inserts a new dimension of size 1 at dimension 1 (the column dimension). This creates a tensor with shape `(batch_size, 1)`, aligning with the expected decoder input format at the first step.\n",
    "\n",
    "   **b. Decoder Output Reshaping and Detachment (squeeze(-1).detach())**\n",
    "\n",
    "     - After each decoding step, the decoder generates an output.\n",
    "     - We need to compare the decoder output with the corresponding element in the target sequence for calculating the loss during training.\n",
    "     - However, the decoder output might have a shape `(batch_size, target_length, hidden_size)`:\n",
    "\n",
    "       - `batch_size`: Number of samples in the batch.\n",
    "       - `target_length`: Length of the target sequence.\n",
    "       - `hidden_size`: Model's internal hidden state dimension (representing extracted features).\n",
    "\n",
    "     ```python\n",
    "     # Example decoder output after a decoding step\n",
    "     decoder_output = model(decoder_input)  # Model processes decoder input\n",
    "\n",
    "     # Process decoder output for loss calculation\n",
    "     processed_decoder_output = decoder_output.squeeze(-1).detach()\n",
    "     ```\n",
    "\n",
    "     - Explanation:\n",
    "       - `squeeze(-1)` removes the dimension of size 1 at the last dimension (dimension -1). This ensures the decoder output has the same shape `(batch_size, target_length)` as the target sequence, allowing for element-wise comparison during loss calculation. Essentially, it extracts the meaningful content from the last dimension (`hidden_size`) by combining it with the previous dimensions (`batch_size` and `target_length`).\n",
    "       - `.detach()` detaches the processed decoder output from the computational graph. Since we're using the ground truth for teacher forcing and not backpropagating through the decoder output in this step, detaching saves memory and avoids unnecessary computations.\n",
    "\n",
    "**3. Key Points:**\n",
    "\n",
    "- `unsqueeze(1)` prepares the target sequence for teacher forcing by creating an input with the correct shape for the first decoding step.\n",
    "- `squeeze(-1).detach()` processes the decoder output by:\n",
    "   - Removing the unnecessary `hidden_size` dimension for element-wise comparison.\n",
    "   - Detaching the output from the computational graph for teacher forcing efficiency.\n",
    "\n",
    "**In essence, these operations ensure that the decoder receives the appropriate teacher forcing input and that the decoder output is properly shaped for loss calculation during training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long,\\\n",
    "                                   device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input,\n",
    "                                                               decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            \n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing, feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # teacher forcing\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach() # detach from hist as input\n",
    "                \n",
    "        decoder_outputs = torch.cat(decoer_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        # return `None` for consistency in the training loop\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "    \n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad31d4",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/1152PYf.png)  \n",
    "![](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing taining data\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\", True)\n",
    "    \n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    \n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "        \n",
    "        \n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, \n",
    "                                  batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afb64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to print time elapsed and estimated time remaining given the current time and progress %.\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a7f1f",
   "metadata": {},
   "source": [
    "> Change the backend to TkAgg, QtAgg, or WXAgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('QtAgg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual train, evaluation\n",
    "\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
    "\n",
    "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
    "\n",
    "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
    "\n",
    "evaluateAndShowAttention('je suis reellement fiere de vous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a094df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aead68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ced821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fd56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504268e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03422c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f3970f",
   "metadata": {},
   "source": [
    "# preprocess custom text dataset using torchtext \n",
    "https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html\n",
    "\n",
    "Read a dataset\n",
    "\n",
    "Tokenize sentence\n",
    "\n",
    "Apply transforms to sentence\n",
    "\n",
    "Perform bucket batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "zh = spacy.load(\"zh_core_web_sm\")\n",
    "de = spacy.load(\"de_core_news_sm\")\n",
    "fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"data/deu-eng/deu.txt\"\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode=\"rb\")\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter=\"\\t\", as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSample(data_pipe):\n",
    "    for sample in data_pipe:\n",
    "        print(sample)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "showSample(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"\n",
    "    Keeping the first two elements in a tuple\n",
    "    Keeping the first and second columns\n",
    "    \"\"\"\n",
    "    return row[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8112316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "showSample(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ac89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize a German text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for the tokenization\n",
    "print(engTokenize(\"Hello world!!!\"))\n",
    "print(deTokenize(\"Hallo Welt!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(engTokenize(\"Have a good day!!!\"))\n",
    "print(deTokenize(\"Haben Sie einen guten Tag!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795684cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since, our iterator contains\n",
    "    tuple of sentences (source and target), `place` parameters defines for which\n",
    "    index to return the tokens for. `place=0` for source and `place=1` for target\n",
    "    \"\"\"\n",
    "    for english, german in data_iter:\n",
    "        if place == 0:\n",
    "            yield engTokenize(english)\n",
    "        else:\n",
    "            yield deTokenize(german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe, 0),\n",
    "    min_freq=2,\n",
    "    specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
    "    special_first=True\n",
    ")\n",
    "source_vocab.set_default_index(source_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,1),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for vocab\n",
    "print(source_vocab.get_itos()[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalize sentences using vocabulary\n",
    "# convert our sentences to corresponding indices\n",
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_transform = T.Sequential(\n",
    "        # converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997f78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5321c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[798][0]\n",
    "print(f\"Some sentence = \", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence = \", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "    \n",
    "    return (\n",
    "        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n",
    "        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(applyTransform)\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_string = source_vocab.get_itos()\n",
    "for inx_list in temp_list[0]:\n",
    "#     print(index_to_string[index], end=\"\")\n",
    "    output = []\n",
    "    \n",
    "    for inx in inx_list:\n",
    "        print(index_to_string[inx], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 4,\n",
    "    batch_num = 5,\n",
    "    bucket_num = 1,\n",
    "    use_in_batch_shuffle=False,\n",
    "    sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(data_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f351594",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the function to each element in the iterator\n",
    "data_pipe = data_pipe.map(separateSourceTarget)\n",
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55649084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply paddjing\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd304d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  index to string mapping to see how the sequence would look with tokens instead of indices\n",
    "\n",
    "source_index_to_string = source_vocab.get_itos()\n",
    "target_index_to_string = target_vocab.get_itos()\n",
    "\n",
    "\n",
    "def showSomeTransformedSentences(data_pipe):\n",
    "    \"\"\"\n",
    "    Function to show how the sentences look like after applying all transforms.\n",
    "    Here we try to print actual words instead of corresponding index\n",
    "    \"\"\"\n",
    "    for sources,targets in data_pipe:\n",
    "        if sources[0][-1] != 0:\n",
    "            continue # Just to visualize padding of shorter sentences\n",
    "        for i in range(4):\n",
    "            source = \"\"\n",
    "            for token in sources[i]:\n",
    "                source += \" \" + source_index_to_string[token]\n",
    "            target = \"\"\n",
    "            for token in targets[i]:\n",
    "                target += \" \" + target_index_to_string[token]\n",
    "            print(f\"Source: {source}\")\n",
    "            print(f\"Target: {target}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "showSomeTransformedSentences(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f2fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce04c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f51d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32e471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6ff75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "418a1c54",
   "metadata": {},
   "source": [
    "# Text classification with the torchtext library\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "`!pip install -U portalocker>=2.0.0``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe588ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3faa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(AG_NEWS(split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82046ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(next(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79290ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5071a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([\"hello\", \"gears\", \"of\", \"war\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb944056",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ea93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe037801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaf109",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7fbd2",
   "metadata": {},
   "source": [
    "### why offsets\n",
    "\n",
    "Certainly! Here's a concrete example with more than 5 sequences to illustrate offset computation with `cumsum` and slicing:\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "Imagine a batch containing six text sequences of varying lengths:\n",
    "\n",
    "- Sequence 1: \"This is the first sentence\" (length 5)\n",
    "- Sequence 2: \"A shorter sentence\" (length 4)\n",
    "- Sequence 3: \"Another example sentence\" (length 5)\n",
    "- Sequence 4: \"This is sequence number four\" (length 7)\n",
    "- Sequence 5: \"A very brief message\" (length 3)\n",
    "- Sequence 6: \"The final sequence of the batch\" (length 6)\n",
    "\n",
    "**`collate_batch` Function:**\n",
    "\n",
    "1. **Processing Sequences:**\n",
    "\n",
    "   - Processes each sequence using `text_pipeline` (e.g., tokenization), resulting in processed tensors.\n",
    "\n",
    "2. **Creating `offsets`:**\n",
    "\n",
    "   - Keeps track of starting indices by appending the length of each processed sequence:\n",
    "\n",
    "     ```\n",
    "     offsets = [0, 5, 9, 14, 21, 24]\n",
    "     ```\n",
    "\n",
    "     - `offsets[0]` to `offsets[5]` represent the accumulated lengths of sequences 1 to 6, respectively.\n",
    "\n",
    "**Offsets Processing:**\n",
    "\n",
    "1. **Slicing (Optional, depending on implementation):**\n",
    "\n",
    "   - `offsets[:-1] = [0, 5, 9, 14, 21]`: This excludes the last element (total length) if it's present.\n",
    "\n",
    "2. **Converting to Tensor:**\n",
    "\n",
    "   - `torch.tensor(offsets[:-1])`: Converts the sliced `offsets` (or the entire list if not sliced) into a PyTorch tensor.\n",
    "\n",
    "3. **Cumulative Sum:**\n",
    "\n",
    "   - ```\n",
    "     .cumsum(dim=0)\n",
    "     ```\n",
    "\n",
    "     :\n",
    "\n",
    "      Calculates the starting indices for each sequence:\n",
    "\n",
    "     ```\n",
    "     offsets_with_indices = torch.tensor([0, 5, 9, 14, 21, 27]).cumsum(dim=0)\n",
    "     ```\n",
    "\n",
    "     - `offsets_with_indices` becomes `tensor([0, 5, 9, 14, 21, 27])`.\n",
    "\n",
    "**Understanding the Results:**\n",
    "\n",
    "- ```\n",
    "  offsets_with_indices\n",
    "  ```\n",
    "\n",
    "   now holds the absolute starting indices for each sequence within the combined \n",
    "\n",
    "  ```\n",
    "  text_list\n",
    "  ```\n",
    "\n",
    "   tensor:\n",
    "\n",
    "  - Sequence 1: Starts at index `offsets_with_indices[0]` (0).\n",
    "  - Sequence 2: Starts at index `offsets_with_indices[1]` (5).\n",
    "  - Sequence 3: Starts at index `offsets_with_indices[2]` (9).\n",
    "  - And so on...\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Slicing with `offsets[:-1]` addresses the potential issue of including the total length in `offsets`.\n",
    "- The cumulative sum (`cumsum`) provides the correct starting indices for efficient processing of each sequence in the batch.\n",
    "\n",
    "I hope this example with six sequences clarifies the concept of offsets, slicing, and `cumsum` in the context of handling variable-length text data during batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d458835",
   "metadata": {},
   "source": [
    "<u>The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9156e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [],[],[0]\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        \n",
    "        label_list.append(label_pipeline(_label))\n",
    "        \n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "        offsets.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # about cumsum function\n",
    "\n",
    "# import torch\n",
    "\n",
    "# # Sample tensor\n",
    "# tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# # Cumulative sum across rows (dim=0)\n",
    "# cumulative_sum_rows = torch.cumsum(tensor, dim=0)\n",
    "# print(cumulative_sum_rows)  # Output: tensor([[1, 3, 6], [4, 9, 15]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2142b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8,\n",
    "    shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f4d44",
   "metadata": {},
   "source": [
    "The `offsets` parameter plays a crucial role in efficiently handling variable-length text sequences within the `embedding` layer (specifically, the `nn.EmbeddingBag` module). Here's a breakdown of how it works:\n",
    "\n",
    "**`nn.EmbeddingBag` for Variable-Length Sequences:**\n",
    "\n",
    "- The standard `nn.Embedding` module assumes all sequences have the same length.\n",
    "- However, in text classification, sequences often have different lengths.\n",
    "- `nn.EmbeddingBag` overcomes this limitation by efficiently processing variable-length sequences.\n",
    "\n",
    "**How Offsets Help:**\n",
    "\n",
    "1. **Combined Text List (`text`):**\n",
    "\n",
    "   - Imagine a tensor `text` that combines all processed sequences (e.g., tokenized) from the batch into a single list.\n",
    "   - This `text` tensor might contain all the tokens from all sequences, one after another.\n",
    "\n",
    "2. **`offsets`:**\n",
    "\n",
    "   - This parameter is a tensor (often created from a list) that provides the starting index for each sequence within the combined `text` tensor.\n",
    "\n",
    "   - For example,\n",
    "\n",
    "      \n",
    "\n",
    "     ```\n",
    "     offsets = [0, 5, 9]\n",
    "     ```\n",
    "\n",
    "      might indicate that:\n",
    "\n",
    "     - Sequence 1 starts at index 0 in `text`.\n",
    "     - Sequence 2 starts at index 5 in `text`.\n",
    "     - Sequence 3 starts at index 9 in `text`.\n",
    "\n",
    "**`embedding(text, offsets)` in Action:**\n",
    "\n",
    "1. Leveraging Offsets:\n",
    "   - The `embedding` layer (using `nn.EmbeddingBag`) uses `offsets` to identify the relevant subsequence for each sequence within the combined `text` tensor.\n",
    "2. Embedding Calculation:\n",
    "   - For each sequence, it extracts the corresponding subsequence from `text` based on the starting index provided by `offsets`.\n",
    "   - This subsequence represents the sequence's tokens.\n",
    "3. Embedding Bag Operation:\n",
    "   - `nn.EmbeddingBag` then performs embedding on the extracted subsequence, essentially converting each token in the sequence to a dense vector representation.\n",
    "\n",
    "**Benefits of `offsets`:**\n",
    "\n",
    "- Enables efficient processing of variable-length sequences in a single batch.\n",
    "- Reduces memory overhead compared to padding sequences to a fixed length.\n",
    "\n",
    "**Further Explanation:**\n",
    "\n",
    "- The `nn.Embedding` layer likely has a pre-trained weight matrix that maps each word in the vocabulary to a dense embedding vector (e.g., size `embed_dim`).\n",
    "- By looking up the indices of tokens in the subsequence (extracted using `offsets`), the `embedding` layer creates an embedding representation for the entire sequence.\n",
    "\n",
    "**In essence, `offsets` act as a guide for the `embedding` layer to efficiently extract and process individual sequences within the combined `text` tensor, enabling effective text classification even with variable-length text data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a58902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange=.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864731ce",
   "metadata": {},
   "source": [
    "## The AG_NEWS dataset has four labels and therefore the number of classes is four.\n",
    "1 : World   \n",
    "2 : Sports    \n",
    "3 : Business    \n",
    "4 : Sci/Tec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split=\"train\")\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3005718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0,0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d} | {:5d} batches\"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            total_acc, total_count = 0,0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead8f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e8eba4a",
   "metadata": {},
   "source": [
    "# random_split and to_map_style_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb039ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(40)\n",
    "a = random_split(range(10), [0.5, 0.3, 0.2], generator=generator)\n",
    "for group in a:\n",
    "    for each in group:\n",
    "        print(each)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(40)\n",
    "a = random_split(range(10), [3,7], generator=generator)\n",
    "for group in a:\n",
    "    for each in group:\n",
    "        print(each)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_datset, [num_train, lean(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161e1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2885972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcb0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27f73c9",
   "metadata": {},
   "source": [
    "# Language Translation with nn.Transformer and torchtext\n",
    "https://www.statmt.org/wmt16/multimodal-task.html#task1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebe609",
   "metadata": {},
   "source": [
    "### trochtext.datasets\n",
    "\n",
    "Datasets\n",
    "\n",
    "- [Text Classification](https://pytorch.org/text/stable/datasets.html#text-classification)\n",
    "  - [AG_NEWS](https://pytorch.org/text/stable/datasets.html#ag-news)\n",
    "  - [AmazonReviewFull](https://pytorch.org/text/stable/datasets.html#amazonreviewfull)\n",
    "  - [AmazonReviewPolarity](https://pytorch.org/text/stable/datasets.html#amazonreviewpolarity)\n",
    "  - [CoLA](https://pytorch.org/text/stable/datasets.html#cola)\n",
    "  - [DBpedia](https://pytorch.org/text/stable/datasets.html#dbpedia)\n",
    "  - [IMDb](https://pytorch.org/text/stable/datasets.html#imdb)\n",
    "  - [MNLI](https://pytorch.org/text/stable/datasets.html#mnli)\n",
    "  - [MRPC](https://pytorch.org/text/stable/datasets.html#mrpc)\n",
    "  - [QNLI](https://pytorch.org/text/stable/datasets.html#qnli)\n",
    "  - [QQP](https://pytorch.org/text/stable/datasets.html#qqp)\n",
    "  - [RTE](https://pytorch.org/text/stable/datasets.html#rte)\n",
    "  - [SogouNews](https://pytorch.org/text/stable/datasets.html#sogounews)\n",
    "  - [SST2](https://pytorch.org/text/stable/datasets.html#sst2)\n",
    "  - [STSB](https://pytorch.org/text/stable/datasets.html#stsb)\n",
    "  - [WNLI](https://pytorch.org/text/stable/datasets.html#wnli)\n",
    "  - [YahooAnswers](https://pytorch.org/text/stable/datasets.html#yahooanswers)\n",
    "  - [YelpReviewFull](https://pytorch.org/text/stable/datasets.html#yelpreviewfull)\n",
    "  - [YelpReviewPolarity](https://pytorch.org/text/stable/datasets.html#yelpreviewpolarity)\n",
    "- [Language Modeling](https://pytorch.org/text/stable/datasets.html#language-modeling)\n",
    "  - [PennTreebank](https://pytorch.org/text/stable/datasets.html#penntreebank)\n",
    "  - [WikiText-2](https://pytorch.org/text/stable/datasets.html#wikitext-2)\n",
    "  - [WikiText103](https://pytorch.org/text/stable/datasets.html#wikitext103)\n",
    "- [Machine Translation](https://pytorch.org/text/stable/datasets.html#machine-translation)\n",
    "  - [IWSLT2016](https://pytorch.org/text/stable/datasets.html#iwslt2016)\n",
    "  - [IWSLT2017](https://pytorch.org/text/stable/datasets.html#iwslt2017)\n",
    "  - [Multi30k](https://pytorch.org/text/stable/datasets.html#multi30k)\n",
    "- [Sequence Tagging](https://pytorch.org/text/stable/datasets.html#sequence-tagging)\n",
    "  - [CoNLL2000Chunking](https://pytorch.org/text/stable/datasets.html#conll2000chunking)\n",
    "  - [UDPOS](https://pytorch.org/text/stable/datasets.html#udpos)\n",
    "- [Question Answer](https://pytorch.org/text/stable/datasets.html#question-answer)\n",
    "  - [SQuAD 1.0](https://pytorch.org/text/stable/datasets.html#squad-1-0)\n",
    "  - [SQuAD 2.0](https://pytorch.org/text/stable/datasets.html#squad-2-0)\n",
    "- [Unsupervised Learning](https://pytorch.org/text/stable/datasets.html#unsupervised-learning)\n",
    "  - [CC100](https://pytorch.org/text/stable/datasets.html#cc100)\n",
    "  - [EnWik9](https://pytorch.org/text/stable/datasets.html#enwik9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6857b39",
   "metadata": {},
   "source": [
    "Python offers a typing module that provides various data type annotations to improve code readability and maintainability.\n",
    "These annotations don't affect the code's functionality at runtime, but they act as hints for developers and static type checkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e715a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# typing, Iterable, and List\n",
    "from typing import Iterable, List\n",
    "\n",
    "def print_all_items(items: Iterable) -> None:\n",
    "    \"\"\"Prints all items in an iterable object.\"\"\"\n",
    "    for item in items:\n",
    "        print(item)\n",
    "\n",
    "# Example usage\n",
    "my_list: List[int] = [1, 2, 3]\n",
    "print_all_items(my_list)  # This works as the list is iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52663d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# data sourcing and processing\n",
    "\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478c407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9010fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = \"de\"\n",
    "TGT_LANGUAGE = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3841b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7a3a0",
   "metadata": {},
   "source": [
    "```shell\n",
    "pip install -U torchdata\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a82a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source and target language tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98228e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "    \n",
    "    for data_sample in data_iter:\n",
    "        \"\"\"\n",
    "        Yield tokens\n",
    "        \"\"\"\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ec246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0,1,2,3\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f9ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ln - language name\n",
    "\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # training data iterator\n",
    "    train_iter = Multi30k(split=\"train\", \n",
    "                          language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # create torchtext vocab object\n",
    "    \"\"\"\n",
    "    Yield indices\n",
    "    \"\"\"\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, ln),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True\n",
    "    )\n",
    "    \n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when \n",
    "# the token is not found. If not set, it throws ``RuntimeError`` \n",
    "# when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8326c",
   "metadata": {},
   "source": [
    "# Seq2seq - transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c1cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ef627",
   "metadata": {},
   "source": [
    "![](./img/pe.png)   \n",
    "\n",
    "\n",
    "1. **Imagine a numbering system:** Assign a unique number (position index) to each word in the sequence, starting from 0 for the first word.\n",
    "2. **Create a special table (embedding):** This table has the same size (dimensions) as the word embeddings used in the model. Each row in the table represents the positional encoding for a specific position index.\n",
    "3. **Encode position using sine and cosine:**  Instead of directly using the position index, the encoding for each position is created using sine and cosine functions. The value of these functions depends on both the position index and the dimension of the word embedding. This creates a smooth and continuous representation of position.\n",
    "4. **Why sine and cosine?** These functions have periodic properties, meaning their values repeat after a certain interval. This helps the model learn long-range dependencies in the sequence, even if the positions are far apart.\n",
    "5. **Adding the encoding:** The positional encoding for each word (a row from the table) is added to the corresponding word embedding. This injects information about the word's position into the word representation.\n",
    "\n",
    "**Think of it like adding directional arrows to each word:**\n",
    "\n",
    "- The arrow's strength (amplitude) depends on the position index (using sine and cosine).\n",
    "- The direction (up/down) alternates for even and odd positions.\n",
    "- By adding these arrows to the word embeddings, the model can learn not only the meaning of each word but also its relative position within the sentence.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Positional encoding helps the model understand the order of words in a sequence.\n",
    "- It uses sine and cosine functions to create a smooth and continuous representation of position.\n",
    "- The encoding is added to the word embeddings, enriching them with positional information.  \n",
    "\n",
    "$$PE_{(pos, 2i)} = sin \\bigg(\\frac{pos}{10000^{2i/d_{model}}}\\bigg)$$     \n",
    "$$PE_{(pos, 2i+1)} = cos \\bigg(\\frac{pos}{10000^{2i/d_{model}}}\\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2138cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.82185619,  0.56969501,  0.8019618 ,\n",
       "         0.59737533],\n",
       "       [ 0.90929743, -0.41614684,  0.93641474, -0.35089519,  0.95814438,\n",
       "        -0.28628544],\n",
       "       [ 0.14112001, -0.9899925 ,  0.24508542, -0.96950149,  0.34278182,\n",
       "        -0.93941504],\n",
       "       [-0.7568025 , -0.65364362, -0.65716686, -0.75374513, -0.54860557,\n",
       "        -0.83608129],\n",
       "       [-0.95892427,  0.28366219, -0.99385478,  0.11069182, -0.99822869,\n",
       "        -0.05949362]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization of PE\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "MAX_SEQ_LEN = 128 # maximum length of a sentence  - postion\n",
    "d_model = 512 # word embedding (and positional encoding) dimensions\n",
    "\n",
    "# pre-allocates vectors with zeros\n",
    "PE = np.zeros((MAX_SEQ_LEN, d_model))\n",
    "\n",
    "# for each position, and for each dimension\n",
    "for pos in range(MAX_SEQ_LEN):\n",
    "    for i in range(d_model//2):  # d_model // 2 becuase the PE use pairs of (sin,cos)\n",
    "        theta = pos / (10000 ** ((2*i)/d_model))\n",
    "        PE[pos, 2*i ] = math.sin(theta)\n",
    "        PE[pos, 2*i + 1] = math.cos(theta)\n",
    "        \n",
    "        \n",
    "PE[:6, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56d8b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using for-loop, we can take advantage of NumPy’s parallelizable operations \n",
    "# (inspired by the PyTorch tutorial https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "\n",
    "pos = np.arange(MAX_SEQ_LEN)[:, np.newaxis]\n",
    "# np.arange(0, d_model, 2) generates integers from 0 to d_model - 2\n",
    "div_term = np.exp(np.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "PE[:, 0::2] = np.sin(pos * div_term)\n",
    "PE[:, 1::2] = np.cos(pos * div_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ab97e",
   "metadata": {},
   "source": [
    "# How to derive from the for loop to np vectorizatin manner\n",
    "\n",
    "### 1. Formula-1\n",
    "## ${div-term} = \\frac{pos}{10000 (\\frac{2i}{d_{model}})}$\n",
    "\n",
    "### 2. Formula-2\n",
    "## ${div-term} = {np.exp}\\bigg({np.arange(0, d_{model}, 2)} \\cdot \\bigg(-\\frac{math.log(10000.0)}{d_{model}}\\bigg)\\bigg)$\n",
    "\n",
    "### 3. Understanding logarithms and exponention\n",
    "## $e^{({log_e(a)})} = a$\n",
    "\n",
    "\n",
    "### 4. rewrite the first formula with e raised to the power of logarithm of 10000\n",
    "## $e^{({log_e(\\frac{pos}{10000 (\\frac{2i}{d_{model}})})})} = \\frac{pos}{10000 (\\frac{2i}{d_{model}})}$\n",
    "\n",
    "\n",
    "### 5. rewrite the second formula \n",
    "## $-\\frac{math.log(10000.0)}{d_{model}} \\rightarrow  -\\frac{math.log(10000.0)}{d_{model}} \\cdot {d_{model}} = log(d_{model}) + log(-{math.log(10000.0)})$\n",
    "\n",
    "\n",
    "### 6. raising `e` to power of both expressions\n",
    "\n",
    "# $$e^{{\\log(10000) \\cdot \\left( \\frac{2i}{d_{model}} \\right)}}$$\n",
    "# $$e^{{\\left( d_{model} \\cdot \\frac{math.log(10000.0)}{d_{model}} \\right)} \\cdot (-1)}$$\n",
    "\n",
    "\n",
    "Due to the properties of logarithms and exponentiation:\n",
    "\n",
    "- The logarithm part in both expressions cancels out (as raising `e` to the power of its own logarithm results in 1).\n",
    "- The negative sign in the second expression flips the result, effectively matching the behavior of raising 10000 to a negative power in Formula 1.\n",
    "\n",
    "Therefore, both expressions, when raised to `e` (the base of the natural logarithm), yield the same result. This result is the scaling factor for the specific even dimension (`i`).\n",
    "\n",
    "\n",
    "\n",
    "# https://ai.stackexchange.com/questions/41670/why-use-exponential-and-log-in-positional-encoding-of-transformer  \n",
    "\n",
    "\n",
    "# $$PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "# $$PE_{(pos, 2i)} = cos(\\frac{pos}{10000^{2i/d_{model}}})$$  \n",
    "\n",
    "\n",
    "# $\\because x=math.exp (log_e(x)) \\& log x^a = a log x$\n",
    "# $\\therefore \\frac{pos}{10000^{2i/d_{model}}} = exp(log(pos) - \\frac{2i}{d_{model}}log(10000))$\n",
    "\n",
    "\n",
    ">But perhaps you are interested in the question as to why the positional encodings are of this fairly whacky form. My understanding of the main intuitions are that sines and cosines interact really nicely with translation due to their periodicity, and by using exponentially spaced 'frequencies' one can extract signals for interactions at a large number of different 'length scales'; see this excellent blog post and associated links for further explanation. But as I understand it, the main reason this clever idea is so widely used is because it works so well in practice.  https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/code/lianghsunhuang/positional-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6487a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "164ad91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size:int,\n",
    "                 dropout:float,\n",
    "                 maxlen: int=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # den - denominator\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
    "        \n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0),:])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f634fb",
   "metadata": {},
   "source": [
    "Multiplying the embedding vectors by math.sqrt(d_model) essentially scales their initial values. This scaling helps address the issues mentioned above: Normalization: By dividing the variance of the initial values by d_model, the gradients tend to have a more manageable magnitude during backpropagation, improving learning efficiency. Activation Functions: Scaling the initial values ensures they are within a range where activation functions can operate effectively, allowing for more nuanced gradients during training. Alternative Initializations:\n",
    "\n",
    "While math.sqrt(d_model) is a common scaling factor, it's not the only approach. Some researchers use other techniques like uniform initialization within a specific range or initialization based on pre-trained word embeddings from sources like Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "201da913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef57d22",
   "metadata": {},
   "source": [
    "```python\n",
    " def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                src_is_causal: Optional[bool] = None, tgt_is_causal: Optional[bool] = None,\n",
    "                memory_is_causal: bool = False) -> Tensor:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86804992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int, \n",
    "        num_decoder_layers: int,\n",
    "        emb_size: int,\n",
    "        nhead: int, \n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        dim_feedforward: int=512,\n",
    "        dropout: float=0.1\n",
    "    ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model = emb_size,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        trg: Tensor,\n",
    "        src_mask: Tensor,\n",
    "        tgt_mask: Tensor,\n",
    "        src_padding_mask: Tensor,\n",
    "        tgt_padding_mask: Tensor,\n",
    "        memory_key_padding_mask: Tensor\n",
    "    ):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            None,  # memory mask\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            memory_key_padding_mask\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "            self.src_tok_emb(src)  # token emb + positional emb\n",
    "        ), src_mask)\n",
    "    \n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d621d",
   "metadata": {},
   "source": [
    "During training, we need a subsequent word mask that will prevent the model from looking into the future words when making predictions  \n",
    "\n",
    "\n",
    "\n",
    "**Square Subsequent Mask:**\n",
    "\n",
    "- A square subsequent mask is a binary matrix (often of size `sequence_length x sequence_length`) used to ensure the model only attends to past or current positions (including itself) during self-attention.\n",
    "- Here's how it works:\n",
    "  - All elements on the main diagonal (i, i) of the mask are set to 1, allowing the model to attend to the word itself (attend to its own embedding).\n",
    "  - All elements **above** the main diagonal (i, j where i < j) are set to 0, preventing the model from attending to future positions (words that come later in the sequence).\n",
    "  - The elements **below** the main diagonal (i, j where i > j) can be set to 1 (depending on the specific implementation). This allows the model to attend to past positions (words that came earlier in the sequence).\n",
    "  \n",
    "```\n",
    "| 1 | 0 | 0 | 0 | 0 | (Word 1 can attend to itself)\n",
    "| 1 | 1 | 0 | 0 | 0 | (Word 2 can attend to Word 1 and itself)\n",
    "| 1 | 1 | 1 | 0 | 0 | (Word 3 can attend to Word 1, Word 2, and itself)\n",
    "| 1 | 1 | 1 | 1 | 0 | (Word 4 can attend to Word 1, Word 2, Word 3, and itself)\n",
    "| 1 | 1 | 1 | 1 | 1 | (Word 5 can attend to all previous words and itself)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c128ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, \n",
    "                                                                          float(1.0))\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9617a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "    \n",
    "    # decoder masking\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "    \n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0,1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0,1)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e6d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d33dd63",
   "metadata": {},
   "source": [
    "# subsequent mask test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdfc94a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequent_mask = np.triu(np.ones(10), k=1).astype('uint8')\n",
    "subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518f3d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(subsequent_mask) == 0  # subtle way to yield decoder masking with bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da612a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e2b2c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (torch.triu(torch.ones((10, 10), device=DEVICE)) == 1).transpose(0, 1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e97ab91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1 = mask.float().masked_fill(mask == 0, float('-inf'))\n",
    "mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02fc0a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask2 = mask1.masked_fill(mask == 1, float(0.0))\n",
    "mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a89518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad42d36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (torch.triu(torch.ones((10, 10), device=DEVICE)) == 1).transpose(0, 1)\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "mask    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fd33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e97d466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (torch.triu(torch.ones(10,10)) == 1).transpose(0, 1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcd7a96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.float().masked_fill(mask==0, float(\"-inf\"))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17b39151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3a0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc77cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21604d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f8e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cd4fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collation\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b7c9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "344325ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(transformer, optimizer)\n\u001b[1;32m      7\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m      8\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(transformer)\n",
      "Cell \u001b[0;32mIn[31], line 26\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 26\u001b[0m     losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(train_dataloader))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e904d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae3f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
