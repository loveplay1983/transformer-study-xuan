{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reformer 2/4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB72iZ709B5e0yz50GUsCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5f4343ae72d49ddbfa3f09d40356fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_afc7db77344c4a74b2be9deebaf193e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0763a5da0d4441119d5d7efbdf7d0344",
              "IPY_MODEL_2eb9ca3bbf5c47cf916db94b60cd1306"
            ]
          }
        },
        "afc7db77344c4a74b2be9deebaf193e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0763a5da0d4441119d5d7efbdf7d0344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0f46f57e27c34e44812af5477092664e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1279,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1279,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e0260d875ad4af8a138797d620fd55b"
          }
        },
        "2eb9ca3bbf5c47cf916db94b60cd1306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0da1f7cfec54416a10a3b9886bab742",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.28k/1.28k [01:28&lt;00:00, 14.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a1365461f8a4cae8660ec0ff2400bbf"
          }
        },
        "0f46f57e27c34e44812af5477092664e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e0260d875ad4af8a138797d620fd55b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0da1f7cfec54416a10a3b9886bab742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a1365461f8a4cae8660ec0ff2400bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Reformer_2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJOlM8rEC2SA",
        "colab_type": "text"
      },
      "source": [
        "# **The Reformer - Pushing the limits of language modeling**\n",
        "\n",
        "***How the Reformer uses less than 8GB of RAM to train on sequences of half a million tokens***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk1ETLlfEMGA",
        "colab_type": "text"
      },
      "source": [
        "The Reformer model as introduced by [Kitaev, Kaiser et al. (2020)](https://arxiv.org/pdf/2001.04451.pdf) is one of the most memory-efficient transformer models for long sequence modeling as of today.\n",
        "\n",
        "Recently, long sequence modeling has experienced a surge of interest as can be seen by the many submissions from this year alone - [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150), [Roy et al. (2020)](https://arxiv.org/abs/2003.05997), [Tay et al.](https://arxiv.org/abs/2002.11296), [Wang et al.](https://arxiv.org/abs/2006.04768) to name  a few. \n",
        "The motivation behind long sequence modeling is that many tasks in NLP, *e.g.* summarization, question answering, require the model to process longer input sequences than models, such as BERT, are able to handle. In tasks that require the model to process a large input sequence, long sequence models do not have to cut the input sequence to avoid memory overflow and thus have been shown to outperform standard \"BERT\"-like models *cf.* [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150). \n",
        "\n",
        "The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a million tokens at once as shown in this [demo](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb). As a comparison, a conventional `bert-base-uncased` model limits the input length to only 512 tokens. In Reformer, each part of the standard transformer architecture is re-engineered to optimize for minimal memory requirement without a significant drop in performance.\n",
        "\n",
        "The memory improvements can be attributed to **4** features which the Reformer authors introduced to the transformer world:\n",
        "\n",
        "1.   **Reformer Self-Attention Layer** - *How to efficiently implement self-attention without being restricted to a local context?*\n",
        "=> see [this colab](https://colab.research.google.com/drive/15oP52_7W5dRcAnbgX3tYADsu4R3cjMIf?usp=sharing).\n",
        "2.  **Chunked Feed Forward Layers** - *How to get a better time-memory trade-off for large feed forward layers?*\n",
        "3.   **Reversible Residual Layers**  - *How to drastically reduce memory consumption in training by a smart residual architecture?*\n",
        "4.   **Axial Positional Encodings** - *How to make positional encodings usable for extremely large input sequences?*\n",
        "\n",
        "The goal of this blog post is to give the reader an **in-depth** understanding of each of the four Reformer features mentioned above. While the explanations are focussed on the Reformer, the reader should get a better intuition under which circumstances each of the four features can be effective for other transformer models as well. \n",
        "The four sections are only loosely connected, so they can very well be read individually.\n",
        "\n",
        "Reformer is part of the ðŸ¤—Transformers library. For all users of the Reformer, it is advised to go through this very detailed blog post to better understand how the model works and how to correctly set its configuration. All equations are accompanied by their equivalent name for the Reformer config, *e.g.* `config.<param_name>`, so that the reader can quickly relate to the official docs and configuration file.\n",
        "\n",
        "**Note**: *Axial Positional Encodings* are not explained in the official Reformer paper, but are extensively used in the official codebase. This blog post gives the first in-depth explanation of Axial Positional Encodings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNs6JrxtglSz",
        "colab_type": "text"
      },
      "source": [
        "## **2. Chunked Feed Forward Layers**\n",
        "\n",
        "Transformer-based models often employ very large feed forward layers after the self-attention layer in parallel. Thereby, this layer can take up a significant amount of the overall memory and sometimes even represent the memory bottleneck of a model.\n",
        "First introduced in the Reformer paper, feed forward chunking is a technique that allows to effectively trade better memory consumption for increased time consumption.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOL5A2FlD-r4",
        "colab_type": "text"
      },
      "source": [
        "### **Chunked Feed Forward Layer in Reformer**\n",
        "\n",
        "In Reformer, the *LSH*- or *local* self-attention layer (review part 1 [here](https://colab.research.google.com/drive/15oP52_7W5dRcAnbgX3tYADsu4R3cjMIf?usp=sharing)) is usually followed by a residual connection, which then defines the first part in a *transformer block*. For more detail on this please refer to this [blog](http://jalammar.github.io/illustrated-transformer/). \n",
        "\n",
        "The output of the first part of the *transformer block*, called *normed self-attention* output can be written as $\\mathbf{\\overline{Z}} = \\mathbf{Z} + \\mathbf{X}$, with $\\mathbf{Z}$ being either $\\mathbf{Z}^{\\text{LSH}}$ or $\\mathbf{Z}^\\text{loc}$ in Reformer.\n",
        "\n",
        "For our example input $\\mathbf{x}_1, \\ldots, \\mathbf{x}_{16}$, we illustrate the normed self-attention output as follows.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/layer_normed_output.png)\n",
        "\n",
        "Now, the second part of a *transformer block* usually consists of two feed forward layers$^{1}$, defined as $\\text{Linear}_{\\text{int}}(\\ldots)$ that processes $\\mathbf{\\overline{Z}}$, to an intermediate output $\\mathbf{Y}_{\\text{int}}$ and $\\text{Linear}_{\\text{out}}(\\ldots)$ that processes the intermediate output to the output $\\mathbf{Y}_{\\text{out}}$. The two feed forward layers can be defined by $\\mathbf{Y}_{\\text{out}} = \\text{Linear}_{\\text{out}}(\\mathbf{Y}_\\text{int}) = \n",
        "\\text{Linear}_{\\text{out}}(\\text{Linear}_{\\text{int}}(\\mathbf{\\overline{Z}}))$.\n",
        "\n",
        "It is important to remember at this point that mathematically the output of a feed forward layer at position $\\mathbf{y}_{\\text{out}, i}$ only depends on the input at this position $\\mathbf{\\overline{y}}_i$. In contrast to the self-attention layer, every output $\\mathbf{y}_{\\text{out}, i}$ is therefore completely independent of all inputs $\\mathbf{\\overline{y}}_{j \\ne i}$ of different positions. \n",
        "\n",
        "Let's illustrate the feed forward layers for $\\mathbf{\\overline{z}}_1, \\ldots, \\mathbf{\\overline{z}}_{16}$.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/feed_forward.png)\n",
        "\n",
        "As can be depicted from the illustration, all input vectors $\\mathbf{\\overline{z}}_i$ are processed by the same feed forward layer in parallel.\n",
        "\n",
        "It becomes interesting when one takes a look at the output dimensions of the feed forward layers. In Reformer, the output dimension of $\\text{Linear}_{\\text{int}}$ is defined as `config.feed_forward_size`, *e.g.* $d_f$, and the output dimension of $\\text{Linear}_{\\text{int}}$ is defined as `config.hidden_size`, *i.e.* $d_h$. \n",
        "\n",
        "The Reformer authors observed that in a transformer model the intermediate dimension $d_f$ usually tends to be much larger than the output dimension$^{2}$ $d_h$. This means that the tensor $\\mathbf{\\mathbf{Y}}_\\text{int}$ of dimension $d_f \\times n$ allocates a significant amount of the total memory and can even become the memory bottleneck.\n",
        "\n",
        "To get a better feeling for the differences in dimensions let's picture the matrices $\\mathbf{Y}_\\text{int}$ and $\\mathbf{Y}_\\text{out}$ for our example.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/feed_forward_matrix.png)\n",
        "\n",
        "It is becoming quite obvious that the tensor $\\mathbf{Y}_\\text{int}$ holds much more memory ($\\frac{d_f}{d_h} \\times n$ as much to be exact) than $\\mathbf{Y}_{\\text{out}}$. But, is it even necessary to compute the full intermediate matrix $\\mathbf{Y}_\\text{int}$ ? Not really, because relevant is only the output matrix $\\mathbf{Y}_\\text{out}$. \n",
        "To trade memory for speed, one can thus chunk the linear layers computation to only process one chunk at the time. Defining `config.chunk_size_feed_forward` as $c_f$, chunked linear layers are defined as $\\mathbf{Y}_{\\text{out}} = \\left[\\mathbf{Y}_{\\text{out}, 1: c_f}, \\ldots, \\mathbf{Y}_{\\text{out}, (n - c_f): n}\\right]$ with $\\mathbf{Y}_{\\text{out}, (c_f * i): (i * c_f + i)} = \\text{Linear}_{\\text{out}}(\\text{Linear}_{\\text{int}}(\\mathbf{\\overline{Z}}_{(c_f * i): (i * c_f + i)}))$. \n",
        "In practice, it just means that the output is incrementally computed and concatenated to avoid having to store the whole intermediate tensor $\\mathbf{Y}_{\\text{int}}$ in memory.\n",
        "\n",
        "Assuming $c_f=1$ for our example we can illustrate the incremental computation of the output for position $i=9$ as follows. \n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/chunked_feed_forward.png)\n",
        "\n",
        "By processing the inputs in chunks of size 1, the only tensors that have to be stored in memory at the same time are $\\mathbf{Y}_\\text{out}$ of a maximum size of $16 \\times d_h$, $\\mathbf{y}_{\\text{int}, i}$ of size $d_f$ and the input $\\mathbf{\\overline{Z}}$ of size $16 \\times d_h$, with $d_h$ being `config.hidden_size`$^{3}$.\n",
        "\n",
        "Finally, it is important to remember that *chunked linear layers* yield a mathematically equivalent output to conventional linear layers and can therefore be applied to all transformer linear layers. Making use of `config.chunk_size_feed_forward` therefore allows a better trade-off between memory and speed in certain use cases.\n",
        "\n",
        "---\n",
        "${}^1$ For a simpler explanation, the layer norm layer which is normally applied to $\\mathbf{\\overline{Z}}$ before being processed by the feed forward layers is omitted for now.\n",
        "\n",
        "${}^2$ In `bert-base-uncased`, *e.g.* the intermediate dimension $d_f$ is with 3072 four times larger than the output dimension $d_h$.\n",
        "\n",
        "${}^3$ As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden_size`.\n",
        "\n",
        "More information on chunked linear / feed forward layers can also be found [here](https://huggingface.co/transformers/glossary.html#feed-forward-chunking) on the ðŸ¤—Transformers docs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8H6l0nwguG6",
        "colab_type": "text"
      },
      "source": [
        "### **Benchmark**\n",
        "\n",
        "Let's test how much memory can be saved by using chunked feed forward layers. Check out this [notebook](https://github.com/huggingface/transformers/blob/master/notebooks/05-benchmark.ipynb) for more detail on benchmarking in Transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "uruiPZSvnTr3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0fea9ec2-b16d-46e1-f6a0-0f1b28c38aa1"
      },
      "source": [
        "#@title Installs and Imports\n",
        "# pip installs\n",
        "!pip -qq install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -qq py3nvml\n",
        "\n",
        "from transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 6.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 40.7MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 39.3MB/s \n",
            "\u001b[?25h  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 3.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa6nMxpCvIFh",
        "colab_type": "text"
      },
      "source": [
        "First, let's compare the default `google/reformer-enwik8` model without chunked feed forward layers to the one with chunked feed forward layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "phw_UOkDnWkC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338,
          "referenced_widgets": [
            "d5f4343ae72d49ddbfa3f09d40356fa1",
            "afc7db77344c4a74b2be9deebaf193e1",
            "0763a5da0d4441119d5d7efbdf7d0344",
            "2eb9ca3bbf5c47cf916db94b60cd1306",
            "0f46f57e27c34e44812af5477092664e",
            "6e0260d875ad4af8a138797d620fd55b",
            "d0da1f7cfec54416a10a3b9886bab742",
            "9a1365461f8a4cae8660ec0ff2400bbf"
          ]
        },
        "outputId": "40fe6f1f-39eb-44d6-bd2c-4452470c4664"
      },
      "source": [
        "config_no_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\")  # no chunk\n",
        "config_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", chunk_size_feed_forward=1)  # feed forward chunk\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[1024, 2048, 4096], batch_sizes=[8], models=[\"Reformer-No-Chunk\", \"Reformer-Chunk\"], no_speed=True, no_env_print=True)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_chunk, config_chunk], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5f4343ae72d49ddbfa3f09d40356fa1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1279.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1 / 2\n",
            "Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.74 GiB free; 9.06 GiB reserved in total by PyTorch)\n",
            "2 / 2\n",
            "Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.24 GiB free; 9.56 GiB reserved in total by PyTorch)\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "      Reformer-No-Chunk              8              1024            4281     \n",
            "      Reformer-No-Chunk              8              2048            7607     \n",
            "      Reformer-No-Chunk              8              4096            N/A      \n",
            "        Reformer-Chunk               8              1024            4309     \n",
            "        Reformer-Chunk               8              2048            7669     \n",
            "        Reformer-Chunk               8              4096            N/A      \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWUt2VZvuHL",
        "colab_type": "text"
      },
      "source": [
        "Interesting, chunked feed forward layers do not seem to help here at all! The reason is that `config.feed_forward_size` is not sufficiently large to become the memory bottleneck. \n",
        "\n",
        "Let's see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kKsvtutaovYp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "263ba73c-21b5-407c-f26a-f236d4989aff"
      },
      "source": [
        "config_no_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", chunk_size_feed_forward=0, num_attention_heads=2, feed_forward_size=16384)  # no chuck\n",
        "config_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", chunk_size_feed_forward=1, num_attention_heads=2, feed_forward_size=16384)  # feed forward chunk\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[1024, 2048, 4096], batch_sizes=[8], models=[\"Reformer-No-Chunk\", \"Reformer-Chunk\"], no_speed=True, no_env_print=True)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_chunk, config_chunk], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "      Reformer-No-Chunk              8              1024            3743     \n",
            "      Reformer-No-Chunk              8              2048            5539     \n",
            "      Reformer-No-Chunk              8              4096            9087     \n",
            "        Reformer-Chunk               8              1024            2973     \n",
            "        Reformer-Chunk               8              2048            3999     \n",
            "        Reformer-Chunk               8              4096            6011     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36IOAWbJxgFG",
        "colab_type": "text"
      },
      "source": [
        "Now a clear decrease in peak memory usage can be seen for longer input sequences. \n",
        "As a conclusion, it should be noted that chunked feed forward layers only make sense for models having few attention heads and large feed forward layers."
      ]
    }
  ]
}