{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583c9578",
   "metadata": {},
   "source": [
    "# This is quite important for practice in ML/DL model building, always start from a sample set from the large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7db6dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import jieba\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2c0d0e4-31cc-46ea-8807-3d1be7734aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Error loading nltk_data/corpora/wordnet: Package\n",
      "[nltk_data]     'nltk_data/corpora/wordnet' not found in index\n",
      "[nltk_data] Error loading nltk_data/corpora/omw-1.4: Package\n",
      "[nltk_data]     'nltk_data/corpora/omw-1.4' not found in index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data] Downloading package cess_cat to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data] Error loading stopwords/english: Package\n",
      "[nltk_data]     'stopwords/english' not found in index\n",
      "[nltk_data] Error loading stopwords/<language_name>: Package\n",
      "[nltk_data]     'stopwords/<language_name>' not found in index\n",
      "[nltk_data] Downloading package gazetteers to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK resource https://www.nltk.org/data.html\n",
    "# Download resources for part-of-speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Download WordNet resources (for tasks like synonym extraction)\n",
    "nltk.download('nltk_data/corpora/wordnet')\n",
    "\n",
    "# Download Open Multilingual WordNet resource\n",
    "nltk.download('nltk_data/corpora/omw-1.4')\n",
    "\n",
    "# Download pre-trained model for sentence tokenization (especially for English)\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#################### Corpora ###############################\n",
    "# Download the Brown Corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Download a collection of English texts from Project Gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Download other corpora (replace names with desired ones)\n",
    "nltk.download('shakespeare')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('cess_cat')\n",
    "\n",
    "############## Stop words ######################################\n",
    "# Download stopwords for a specific language (replace 'english' with the code)\n",
    "nltk.download('stopwords/english')\n",
    "\n",
    "# Download stopwords for other languages (e.g., 'french', 'german')\n",
    "nltk.download('stopwords/<language_name>')\n",
    "\n",
    "############ Additional resource#################\n",
    "# Download gazetteers (geographical name lists)\n",
    "nltk.download('gazetteers')\n",
    "\n",
    "# Download names (personal name lists)\n",
    "nltk.download('names')\n",
    "\n",
    "# Download data for Snowball stemmers\n",
    "nltk.download('snowball_data')\n",
    "\n",
    "# Download Wall Street Journal parsed corpus (for advanced tasks)\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Download sample tweets from Twitter\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08769292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Init parameters\n",
    "\n",
    "UNK = 0 # unknow word-id\n",
    "PAD = 1 # padding word-id\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEBUG = True\n",
    "# DEBUG = False # model building, GPU CUDA is preferred\n",
    "\n",
    "if DEBUG:\n",
    "    EPOCHS = 2\n",
    "    LAYERS = 3\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 128\n",
    "    D_FF = 256\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train_mini.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev_mini.txt\"\n",
    "    SAVE_FILE = \"./save/models/model.pt\"\n",
    "\n",
    "else:\n",
    "    EPOCHS = 20\n",
    "    LAYERS = 6\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 256\n",
    "    D_FF = 1024\n",
    "    DROPUT = .1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev.txt\"\n",
    "    SAVE_FILE = \"./save/models/large_model.pt\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb8a7e-4acb-4422-89f8-68c4de63a7ac",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "1. Load the sentence and tokenize the sentence and add start/end marks(Begin of Sentence /End of Sentence vs BOS/ EOS).\n",
    "2. Build dictionaries including ‘word-to-id’ and inverted dictionary ‘id-to-word’: English and Chinese, ‘word: index}, i.e, {‘english’: 1234}, {1234: ‘english’}.\n",
    "3. Sort the dictionaries to reduce padding.\n",
    "4. Split the dataset into patches for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cdf0a81-7b85-4667-99cd-a93eb882a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    \"\"\"\n",
    "    Add padding to a batch of data\n",
    "    \"\"\"\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([\n",
    "            x, [padding] * (ML - len(x))\n",
    "        ]) if len(x) < ML else x for x in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "725a6d19-70df-45ea-b746-625a22b546ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, train_file, dev_file):\n",
    "        # 1. Read the data and tokenize\n",
    "        self.train_en, self.train_cn = self.load_data(train_file)\n",
    "        self.dev_en, self.dev_cn = self.load_data(dev_file)\n",
    "\n",
    "        # 2. build dictionary: En and CN\n",
    "        self.en_word_dict, self.en_total_words, self.en_index_dict = self.build_dict(self.train_en)\n",
    "        self.cn_word_dict, self.cn_total_words, self.cn_index_dict = self.build_dict(self.train_cn)\n",
    "\n",
    "        # 3. word to id by dictionary\n",
    "        self.train_en, self.train_cn = self.wordToID(self.train_en, self.train_cn, \n",
    "                                                     self.en_word_dict, self.cn_word_dict)\n",
    "        self.dev_en, self.dev_cn = self.wordToID(self.dev_en, self.dev_cn, \n",
    "                                                 self.en_word_dict, self.cn_word_dict)\n",
    "\n",
    "        # 4. batch, padding, and masking\n",
    "        self.train_data = self.splitBatch(self.train_en, self.train_cn, BATCH_SIZE)\n",
    "        self.dev_data = self.splitBatch(self.dev_en, self.dev_cn, BATCH_SIZE)\n",
    "\n",
    "    # Utility functions\n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        read data, tokenize the seence and add start and end marks(bos, eos)\n",
    "        for example:\n",
    "        en = [\n",
    "            [\"BOS\", \"i\", \"love\", \"you\", \"EOS\"],\n",
    "            [\"BOS\", \"me\", \"too\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        cn = [\n",
    "            [\"BOS\", \"我\", \"爱\", \"你\", \"EOS\"],\n",
    "            [\"BOS\", \"我\", \"也\", ,\"是\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        en = []\n",
    "        cn = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                en.append([\"BOS\"] + word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "                cn.append([\"BOS\"] + word_tokenize())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2113139-888a-46f9-953e-3a9f585e7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"I wonder if I hurt Tom's feelings.\t我不知道我是不是伤害了汤姆的感情。\", \"Every one of her songs was a hit.\t她的每首歌都长期备受欢迎。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e18d143c-dded-4792-96ab-aede5ba146bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不知道我是不是伤害了汤姆的感情。\n",
      "她的每首歌都长期备受欢迎。\n"
     ]
    }
   ],
   "source": [
    "test_en = []\n",
    "test_cn = []\n",
    "for i in test:\n",
    "    i = i.strip().split(\"\\t\")\n",
    "    print(i[1])\n",
    "    test_en.append([\"BOS\"] + word_tokenize(i[0].lower()) + [\"EOS\"])\n",
    "    test_cn.append([\"BOS\"] + jieba.cut(i[1]) + [\"EOS\"])\n",
    "\n",
    "print(test_en)\n",
    "print(test_cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94adfc9-63b2-4148-a58c-29f5f7ba0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = jieba.cut(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25e174ff-5c39-4951-a025-d05727bd04ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.509 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 不 知道 我 是不是 伤害 了 汤姆 的 感情 。\n"
     ]
    }
   ],
   "source": [
    "# Define a Chinese sentence\n",
    "text = \"我不知道我是不是伤害了汤姆的感情。\"\n",
    "\n",
    "# Segment the sentence using Jieba\n",
    "seg_list = jieba.cut(text)\n",
    "\n",
    "# Print the tokenized words (separated by spaces)\n",
    "print(\" \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587132ca-02a7-40ed-a98f-b3e558a1a77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
