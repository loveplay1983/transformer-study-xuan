{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583c9578",
   "metadata": {},
   "source": [
    "# This is quite important for practice in ML/DL model building, always start from a sample set from the large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db6dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import jieba\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c0d0e4-31cc-46ea-8807-3d1be7734aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Error loading nltk_data/corpora/wordnet: Package\n",
      "[nltk_data]     'nltk_data/corpora/wordnet' not found in index\n",
      "[nltk_data] Error loading nltk_data/corpora/omw-1.4: Package\n",
      "[nltk_data]     'nltk_data/corpora/omw-1.4' not found in index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package shakespeare is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package cess_cat to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package cess_cat is already up-to-date!\n",
      "[nltk_data] Error loading stopwords/english: Package\n",
      "[nltk_data]     'stopwords/english' not found in index\n",
      "[nltk_data] Error loading stopwords/<language_name>: Package\n",
      "[nltk_data]     'stopwords/<language_name>' not found in index\n",
      "[nltk_data] Downloading package gazetteers to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package gazetteers is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK resource https://www.nltk.org/data.html\n",
    "# Download resources for part-of-speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Download WordNet resources (for tasks like synonym extraction)\n",
    "nltk.download('nltk_data/corpora/wordnet')\n",
    "\n",
    "# Download Open Multilingual WordNet resource\n",
    "nltk.download('nltk_data/corpora/omw-1.4')\n",
    "\n",
    "# Download pre-trained model for sentence tokenization (especially for English)\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#################### Corpora ###############################\n",
    "# Download the Brown Corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Download a collection of English texts from Project Gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Download other corpora (replace names with desired ones)\n",
    "nltk.download('shakespeare')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('cess_cat')\n",
    "\n",
    "############## Stop words ######################################\n",
    "# Download stopwords for a specific language (replace 'english' with the code)\n",
    "nltk.download('stopwords/english')\n",
    "\n",
    "# Download stopwords for other languages (e.g., 'french', 'german')\n",
    "nltk.download('stopwords/<language_name>')\n",
    "\n",
    "############ Additional resource#################\n",
    "# Download gazetteers (geographical name lists)\n",
    "nltk.download('gazetteers')\n",
    "\n",
    "# Download names (personal name lists)\n",
    "nltk.download('names')\n",
    "\n",
    "# Download data for Snowball stemmers\n",
    "nltk.download('snowball_data')\n",
    "\n",
    "# Download Wall Street Journal parsed corpus (for advanced tasks)\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Download sample tweets from Twitter\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08769292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Init parameters\n",
    "\n",
    "UNK = 0 # unknow word-id\n",
    "PAD = 1 # padding word-id\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEBUG = True\n",
    "# DEBUG = False # model building, GPU CUDA is preferred\n",
    "\n",
    "if DEBUG:\n",
    "    EPOCHS = 2\n",
    "    LAYERS = 3\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 128\n",
    "    D_FF = 256\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train_mini.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev_mini.txt\"\n",
    "    SAVE_FILE = \"./save/models/model.pt\"\n",
    "\n",
    "else:\n",
    "    EPOCHS = 20\n",
    "    LAYERS = 6\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 256\n",
    "    D_FF = 1024\n",
    "    DROPUT = .1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev.txt\"\n",
    "    SAVE_FILE = \"./save/models/large_model.pt\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb8a7e-4acb-4422-89f8-68c4de63a7ac",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "1. Load the sentence and tokenize the sentence and add start/end marks(Begin of Sentence /End of Sentence vs BOS/ EOS).\n",
    "2. Build dictionaries including ‘word-to-id’ and inverted dictionary ‘id-to-word’: English and Chinese, ‘word: index}, i.e, {‘english’: 1234}, {1234: ‘english’}.\n",
    "3. Sort the dictionaries to reduce padding.\n",
    "4. Split the dataset into patches for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdf0a81-7b85-4667-99cd-a93eb882a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    \"\"\"\n",
    "    Add padding to a batch of data\n",
    "    \"\"\"\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([\n",
    "            x, [padding] * (ML - len(x))\n",
    "        ]) if len(x) < ML else x for x in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "725a6d19-70df-45ea-b746-625a22b546ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, train_file, dev_file):\n",
    "        # 1. Read the data and tokenize\n",
    "        self.train_en, self.train_cn = self.load_data(train_file)\n",
    "        self.dev_en, self.dev_cn = self.load_data(dev_file)\n",
    "\n",
    "        # 2. build dictionary: En and CN\n",
    "        self.en_word_dict, self.en_total_words, self.en_index_dict = self.build_dict(self.train_en)\n",
    "        self.cn_word_dict, self.cn_total_words, self.cn_index_dict = self.build_dict(self.train_cn)\n",
    "\n",
    "        # 3. word to id by dictionary\n",
    "        self.train_en, self.train_cn = self.wordToID(self.train_en, self.train_cn, \n",
    "                                                     self.en_word_dict, self.cn_word_dict)\n",
    "        self.dev_en, self.dev_cn = self.wordToID(self.dev_en, self.dev_cn, \n",
    "                                                 self.en_word_dict, self.cn_word_dict)\n",
    "\n",
    "        # 4. batch, padding, and masking\n",
    "        self.train_data = self.splitBatch(self.train_en, self.train_cn, BATCH_SIZE)\n",
    "        self.dev_data = self.splitBatch(self.dev_en, self.dev_cn, BATCH_SIZE)\n",
    "\n",
    "    # Utility functions\n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        read data, tokenize the seence and add start and end marks(bos, eos)\n",
    "        for example:\n",
    "        en = [\n",
    "            [\"BOS\", \"i\", \"love\", \"you\", \"EOS\"],\n",
    "            [\"BOS\", \"me\", \"too\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        cn = [\n",
    "            [\"BOS\", \"我\", \"爱\", \"你\", \"EOS\"],\n",
    "            [\"BOS\", \"我\", \"也\", ,\"是\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        en = []\n",
    "        cn = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                en.append([\"BOS\"] + word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "                cn.append([\"BOS\"] + word_tokenize(\" \".join([w for w in line[1]])) + [\"EOS\"])\n",
    "        return en, cn\n",
    "    \n",
    "    def build_dict(self, sentences, max_words = 50000):\n",
    "        \"\"\"\n",
    "        sentences: list of word list\n",
    "        build dictionary as {key(word): value(id)}\n",
    "        \"\"\"\n",
    "        word_count = Counter()\n",
    "        for setence in sentences:\n",
    "            for s in sentence:\n",
    "                word_count[s] += 1\n",
    "                \n",
    "        ls = word_count.most_common(max_words)\n",
    "        total_words = len(ls) + 2 # BOS + EOS = 2\n",
    "        word_dict = {w[0]: index + 2  for index, w in enumerate(ls)}\n",
    "        word_dict[\"UNK\"] = UNK\n",
    "        word_dict[\"PAD\"] = PAD\n",
    "        # inverted index:  {key(id): value(word)}\n",
    "        index_dict = {v: k for k, v in word_dict.items()}\n",
    "        return word_dict, total_words, index_dict\n",
    "        \n",
    "        \n",
    "    def wordToID(self, en, cn, en_dict, cn_dict, sort=True):\n",
    "        \"\"\"\n",
    "        convert input/output word lists to id lists\n",
    "        use input word list length to sort, reduce padding\n",
    "        \"\"\"\n",
    "        length = len(en)\n",
    "        out_en_ids = [[en_dict.get(w, 0) for w in sent] for sent in en]\n",
    "        out_cn_ids = [[cn_dict.get(w, 0) for w in sent] for sent in cn]\n",
    "        \n",
    "        def len_argsort(seq):\n",
    "            \"\"\"\n",
    "            get sorted index w.r.t length.\n",
    "            \"\"\"\n",
    "            \n",
    "            return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "        \n",
    "        if sort:\n",
    "            sorted_index = len_argsort(out_en_ids) # English\n",
    "            out_en_ids = [out_en_ids[id] for id in sorted_index]\n",
    "            out_cn_ids = [out_cn_ids[id] for id in sorted_index]\n",
    "            \n",
    "        return out_en_ids, out_cn_Ids\n",
    "    \n",
    "    def splitBatch(self, en, cn, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        get data into batches\n",
    "        \"\"\"\n",
    "        idx_list = np.arange(0, len(en), batch_size) # start, stop, step\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx_list)\n",
    "\n",
    "        batch_indexs = []\n",
    "        for idx in idx_list:\n",
    "            # batch index between current index and the min index o\n",
    "            batch_indexs.append(np.arange(idx, min(idx+batch_size, len(en)))) \n",
    "\n",
    "        batches = []\n",
    "        for batch_index in batch_indexs:\n",
    "            batch_en = [en[index] for index in batch_index]\n",
    "            batch_cn = [cn[index] for index in batch_index]\n",
    "            # paddings: batch, batch_size, batch_maxlen\n",
    "            batch_cn = seq_padding(batch_cn)\n",
    "            batch_en = seq_padding(batch_en)\n",
    "            # Batch class will be defined later which is the masking batch of data during training\n",
    "            # \"Object for holding a batch of data with mask during training.\"\n",
    "            batches.append(Batch(batch_en, batch_cn)) \n",
    "            \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa805da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Batch:\n",
    "#     \"Object for holding a batch of data with mask during training.\"\n",
    "#     def __init__(self, src, trg=None, pad=0):\n",
    "#         # convert words id to long format.  \n",
    "#         src = torch.from_numpy(src).to(DEVICE).long()\n",
    "#         trg = torch.from_numpy(trg).to(DEVICE).long()\n",
    "#         self.src = src\n",
    "#         # get the padding postion binary mask\n",
    "#         # change the matrix shape to  1×seq.length\n",
    "#         self.src_mask = (src != pad).unsqueeze(-2)\n",
    "#         # 如果输出目标不为空，则需要对decoder要使用到的target句子进行mask\n",
    "#         if trg is not None:\n",
    "#             # decoder input from target \n",
    "#             self.trg = trg[:, :-1]\n",
    "#             # decoder target from trg \n",
    "#             self.trg_y = trg[:, 1:]\n",
    "#             # add attention mask to decoder input  \n",
    "#             self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "#             # check decoder output padding number\n",
    "#             self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "#     # Mask \n",
    "#     @staticmethod\n",
    "#     def make_std_mask(tgt, pad):\n",
    "#         \"Create a mask to hide padding and future words.\"\n",
    "#         tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "#         tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "#         return tgt_mask # subsequent_mask is defined in 'decoder' section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edccc0-f34b-487b-b121-24a80ac9d526",
   "metadata": {},
   "source": [
    "Understanding Initialization:\n",
    "\n",
    "When you create an nn.Embedding layer, it initializes a lookup table with random embedding vectors for each word in the vocabulary.\n",
    "These initial vectors have a specific dimensionality (d_model) but their values are randomly chosen within a certain range.\n",
    "Normalization and Gradient Vanishing:\n",
    "\n",
    "Without the math.sqrt(d_model) factor, the initial values of the embedding vectors can have a large magnitude (very high or very low values).\n",
    "This can lead to two potential issues:\n",
    "Normalization: If the initial values have a large magnitude, the gradients during training might become very small when backpropagated through the network. This is known as the vanishing gradient problem, which can hinder the learning process.\n",
    "Activation Functions: If the network uses activation functions with bounded outputs (like sigmoid or tanh), large initial values can cause these activations to saturate, effectively making them insensitive to further changes.\n",
    "The Role of math.sqrt(d_model):\n",
    "\n",
    "Multiplying the embedding vectors by math.sqrt(d_model) essentially scales their initial values. This scaling helps address the issues mentioned above:\n",
    "Normalization: By dividing the variance of the initial values by d_model, the gradients tend to have a more manageable magnitude during backpropagation, improving learning efficiency.\n",
    "Activation Functions: Scaling the initial values ensures they are within a range where activation functions can operate effectively, allowing for more nuanced gradients during training.\n",
    "Alternative Initializations:\n",
    "\n",
    "While math.sqrt(d_model) is a common scaling factor, it's not the only approach. Some researchers use other techniques like uniform initialization within a specific range or initialization based on pre-trained word embeddings from sources like Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b5975e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output embeddngs\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # lut -> lookup table\n",
    "        self.lut = nn.Embeddings(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return x's embedding vector (times math.sqrt(d_model))\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc956b",
   "metadata": {},
   "source": [
    "# Positional encoding\n",
    "[max_sequence_len, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1da0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3e612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
