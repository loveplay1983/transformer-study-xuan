{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583c9578",
   "metadata": {},
   "source": [
    "# This is quite important for practice in ML/DL model building, always start from a sample set from the large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db6dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import jieba\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c0d0e4-31cc-46ea-8807-3d1be7734aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Error loading nltk_data/corpora/wordnet: Package\n",
      "[nltk_data]     'nltk_data/corpora/wordnet' not found in index\n",
      "[nltk_data] Error loading nltk_data/corpora/omw-1.4: Package\n",
      "[nltk_data]     'nltk_data/corpora/omw-1.4' not found in index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data] Downloading package cess_cat to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data] Error loading stopwords/english: Package\n",
      "[nltk_data]     'stopwords/english' not found in index\n",
      "[nltk_data] Error loading stopwords/<language_name>: Package\n",
      "[nltk_data]     'stopwords/<language_name>' not found in index\n",
      "[nltk_data] Downloading package gazetteers to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK resource https://www.nltk.org/data.html\n",
    "# Download resources for part-of-speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Download WordNet resources (for tasks like synonym extraction)\n",
    "nltk.download('nltk_data/corpora/wordnet')\n",
    "\n",
    "# Download Open Multilingual WordNet resource\n",
    "nltk.download('nltk_data/corpora/omw-1.4')\n",
    "\n",
    "# Download pre-trained model for sentence tokenization (especially for English)\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#################### Corpora ###############################\n",
    "# Download the Brown Corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Download a collection of English texts from Project Gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Download other corpora (replace names with desired ones)\n",
    "nltk.download('shakespeare')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('cess_cat')\n",
    "\n",
    "############## Stop words ######################################\n",
    "# Download stopwords for a specific language (replace 'english' with the code)\n",
    "nltk.download('stopwords/english')\n",
    "\n",
    "# Download stopwords for other languages (e.g., 'french', 'german')\n",
    "nltk.download('stopwords/<language_name>')\n",
    "\n",
    "############ Additional resource#################\n",
    "# Download gazetteers (geographical name lists)\n",
    "nltk.download('gazetteers')\n",
    "\n",
    "# Download names (personal name lists)\n",
    "nltk.download('names')\n",
    "\n",
    "# Download data for Snowball stemmers\n",
    "nltk.download('snowball_data')\n",
    "\n",
    "# Download Wall Street Journal parsed corpus (for advanced tasks)\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Download sample tweets from Twitter\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08769292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Init parameters\n",
    "\n",
    "UNK = 0 # unknow word-id\n",
    "PAD = 1 # padding word-id\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEBUG = True\n",
    "# DEBUG = False # model building, GPU CUDA is preferred\n",
    "\n",
    "if DEBUG:\n",
    "    EPOCHS = 2\n",
    "    LAYERS = 3\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 128\n",
    "    D_FF = 256\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train_mini.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev_mini.txt\"\n",
    "    SAVE_FILE = \"./save/models/model.pt\"\n",
    "\n",
    "else:\n",
    "    EPOCHS = 20\n",
    "    LAYERS = 6\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 256\n",
    "    D_FF = 1024\n",
    "    DROPUT = .1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev.txt\"\n",
    "    SAVE_FILE = \"./save/models/large_model.pt\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb8a7e-4acb-4422-89f8-68c4de63a7ac",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "1. Load the sentence and tokenize the sentence and add start/end marks(Begin of Sentence /End of Sentence vs BOS/ EOS).\n",
    "2. Build dictionaries including ‘word-to-id’ and inverted dictionary ‘id-to-word’: English and Chinese, ‘word: index}, i.e, {‘english’: 1234}, {1234: ‘english’}.\n",
    "3. Sort the dictionaries to reduce padding.\n",
    "4. Split the dataset into patches for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cdf0a81-7b85-4667-99cd-a93eb882a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    \"\"\"\n",
    "    Add padding to a batch of data\n",
    "    \"\"\"\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([\n",
    "            x, [padding] * (ML - len(x))\n",
    "        ]) if len(x) < ML else x for x in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "725a6d19-70df-45ea-b746-625a22b546ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, train_file, dev_file):\n",
    "        # 1. Read the data and tokenize\n",
    "        self.train_en, self.train_cn = self.load_data(train_file)\n",
    "        self.dev_en, self.dev_cn = self.load_data(dev_file)\n",
    "\n",
    "        # 2. build dictionary: En and CN\n",
    "        self.en_word_dict, self.en_total_words, self.en_index_dict = self.build_dict(self.train_en)\n",
    "        self.cn_word_dict, self.cn_total_words, self.cn_index_dict = self.build_dict(self.train_cn)\n",
    "\n",
    "        # 3. word to id by dictionary\n",
    "        self.train_en, self.train_cn = self.wordToID(self.train_en, self.train_cn, \n",
    "                                                     self.en_word_dict, self.cn_word_dict)\n",
    "        self.dev_en, self.dev_cn = self.wordToID(self.dev_en, self.dev_cn, \n",
    "                                                 self.en_word_dict, self.cn_word_dict)\n",
    "\n",
    "        # 4. batch, padding, and masking\n",
    "        self.train_data = self.splitBatch(self.train_en, self.train_cn, BATCH_SIZE)\n",
    "        self.dev_data = self.splitBatch(self.dev_en, self.dev_cn, BATCH_SIZE)\n",
    "\n",
    "    # Utility functions\n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        read data, tokenize the seence and add start and end marks(bos, eos)\n",
    "        for example:\n",
    "        en = [\n",
    "            [\"BOS\", \"i\", \"love\", \"you\", \"EOS\"],\n",
    "            [\"BOS\", \"me\", \"too\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        cn = [\n",
    "            [\"BOS\", \"我\", \"爱\", \"你\", \"EOS\"],\n",
    "            [\"BOS\", \"我\", \"也\", ,\"是\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        en = []\n",
    "        cn = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                en.append([\"BOS\"] + word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "                cn.append([\"BOS\"] + word_tokenize(\" \".join([w for w in line[1]])) + [\"EOS\"])\n",
    "        return en, cn\n",
    "    \n",
    "    def build_dict(self, sentences, max_words = 50000):\n",
    "        \"\"\"\n",
    "        sentences: list of word list\n",
    "        build dictionary as {key(word): value(id)}\n",
    "        \"\"\"\n",
    "        word_count = Counter()\n",
    "        for setence in sentences:\n",
    "            for s in sentence:\n",
    "                word_count[s] += 1\n",
    "                \n",
    "        ls = word_count.most_common(max_words)\n",
    "        total_words = len(ls) + 2 # BOS + EOS = 2\n",
    "        word_dict = {w[0]: index + 2  for index, w in enumerate(ls)}\n",
    "        word_dict[\"UNK\"] = UNK\n",
    "        word_dict[\"PAD\"] = PAD\n",
    "        # inverted index:  {key(id): value(word)}\n",
    "        index_dict = {v: k for k, v in word_dict.items()}\n",
    "        return word_dict, total_words, index_dict\n",
    "        \n",
    "        \n",
    "    def wordToID(self, en, cn, en_dict, cn_dict, sort=True):\n",
    "        \"\"\"\n",
    "        convert input/output word lists to id lists\n",
    "        use input word list length to sort, reduce padding\n",
    "        \"\"\"\n",
    "        length = len(en)\n",
    "        out_en_ids = [[en_dict.get(w, 0) for w in sent] for sent in en]\n",
    "        out_cn_ids = [[cn_dict.get(w, 0) for w in sent] for sent in cn]\n",
    "        \n",
    "        def len_argsort(seq):\n",
    "            \"\"\"\n",
    "            get sorted index w.r.t length.\n",
    "            \"\"\"\n",
    "            \n",
    "            return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "        \n",
    "        if sort:\n",
    "            sorted_index = len_argsort(out_en_ids) # English\n",
    "            out_en_ids = [out_en_ids[id] for id in sorted_index]\n",
    "            out_cn_ids = [out_cn_ids[id] for id in sorted_index]\n",
    "            \n",
    "        return out_en_ids, out_cn_Ids\n",
    "    \n",
    "    def splitBatch(self, en, cn, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        get data into batches\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa805da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46024a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5975e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdba0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5900ad82",
   "metadata": {},
   "source": [
    "### Test with jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e18d143c-dded-4792-96ab-aede5ba146bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不知道我是不是伤害了汤姆的感情。\n",
      "她的每首歌都长期备受欢迎。\n",
      "[['BOS', 'i', 'wonder', 'if', 'i', 'hurt', 'tom', \"'s\", 'feelings', '.', 'EOS'], ['BOS', 'every', 'one', 'of', 'her', 'songs', 'was', 'a', 'hit', '.', 'EOS']]\n",
      "[['BOS', '我', '不', '知道', '我', '是不是', '伤害', '了', '汤姆', '的', '感情', '。', 'EOS'], ['BOS', '她', '的', '每首歌', '都', '长期', '备受', '欢迎', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "# Test with jieba\n",
    "test = [\"I wonder if I hurt Tom's feelings.\t我不知道我是不是伤害了汤姆的感情。\", \n",
    "        \"Every one of her songs was a hit.\t她的每首歌都长期备受欢迎。\"]\n",
    "test_en = []\n",
    "test_cn = []\n",
    "for i in test:\n",
    "    i = i.strip().split(\"\\t\")\n",
    "    print(i[1])\n",
    "    test_en.append([\"BOS\"] + word_tokenize(i[0].lower()) + [\"EOS\"])\n",
    "#     test_cn.append([\"BOS\"] + work_tokenize(\" \".join([w for w in i[1]])) + [\"EOS\"])\n",
    "    cn_seg_list = jieba.cut(i[1])\n",
    "    test_cn.append([\"BOS\"] + [w for w in cn_seg_list] + [\"EOS\"])\n",
    "\n",
    "print(test_en)\n",
    "print(test_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88017394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43493366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1da0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3e612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
