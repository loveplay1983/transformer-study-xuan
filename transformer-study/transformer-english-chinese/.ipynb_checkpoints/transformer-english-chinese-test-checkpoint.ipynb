{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583c9578",
   "metadata": {},
   "source": [
    "# This is quite important for practice in ML/DL model building, always start from a sample set from the large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db6dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import jieba\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c0d0e4-31cc-46ea-8807-3d1be7734aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Error loading nltk_data/corpora/wordnet: Package\n",
      "[nltk_data]     'nltk_data/corpora/wordnet' not found in index\n",
      "[nltk_data] Error loading nltk_data/corpora/omw-1.4: Package\n",
      "[nltk_data]     'nltk_data/corpora/omw-1.4' not found in index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package shakespeare is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package cess_cat to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package cess_cat is already up-to-date!\n",
      "[nltk_data] Error loading stopwords/english: Package\n",
      "[nltk_data]     'stopwords/english' not found in index\n",
      "[nltk_data] Error loading stopwords/<language_name>: Package\n",
      "[nltk_data]     'stopwords/<language_name>' not found in index\n",
      "[nltk_data] Downloading package gazetteers to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package gazetteers is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/loveplay1983/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK resource https://www.nltk.org/data.html\n",
    "# Download resources for part-of-speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Download WordNet resources (for tasks like synonym extraction)\n",
    "nltk.download('nltk_data/corpora/wordnet')\n",
    "\n",
    "# Download Open Multilingual WordNet resource\n",
    "nltk.download('nltk_data/corpora/omw-1.4')\n",
    "\n",
    "# Download pre-trained model for sentence tokenization (especially for English)\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#################### Corpora ###############################\n",
    "# Download the Brown Corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Download a collection of English texts from Project Gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Download other corpora (replace names with desired ones)\n",
    "nltk.download('shakespeare')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('cess_cat')\n",
    "\n",
    "############## Stop words ######################################\n",
    "# Download stopwords for a specific language (replace 'english' with the code)\n",
    "nltk.download('stopwords/english')\n",
    "\n",
    "# Download stopwords for other languages (e.g., 'french', 'german')\n",
    "nltk.download('stopwords/<language_name>')\n",
    "\n",
    "############ Additional resource#################\n",
    "# Download gazetteers (geographical name lists)\n",
    "nltk.download('gazetteers')\n",
    "\n",
    "# Download names (personal name lists)\n",
    "nltk.download('names')\n",
    "\n",
    "# Download data for Snowball stemmers\n",
    "nltk.download('snowball_data')\n",
    "\n",
    "# Download Wall Street Journal parsed corpus (for advanced tasks)\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Download sample tweets from Twitter\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08769292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Init parameters\n",
    "\n",
    "UNK = 0 # unknow word-id\n",
    "PAD = 1 # padding word-id\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEBUG = True\n",
    "# DEBUG = False # model building, GPU CUDA is preferred\n",
    "\n",
    "if DEBUG:\n",
    "    EPOCHS = 2\n",
    "    LAYERS = 3\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 128\n",
    "    D_FF = 256\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train_mini.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev_mini.txt\"\n",
    "    SAVE_FILE = \"./save/models/model.pt\"\n",
    "\n",
    "else:\n",
    "    EPOCHS = 20\n",
    "    LAYERS = 6\n",
    "    H_NUM = 8\n",
    "    D_MODEL = 256\n",
    "    D_FF = 1024\n",
    "    DROPUT = .1\n",
    "    MAX_LENGTH = 60\n",
    "    TRAIN_FILE = \"./data/nmt/en-cn/train.txt\"\n",
    "    DEV_FILE = \"./data/nmt/en-cn/dev.txt\"\n",
    "    SAVE_FILE = \"./save/models/large_model.pt\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb8a7e-4acb-4422-89f8-68c4de63a7ac",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "1. Load the sentence and tokenize the sentence and add start/end marks(Begin of Sentence /End of Sentence vs BOS/ EOS).\n",
    "2. Build dictionaries including ‘word-to-id’ and inverted dictionary ‘id-to-word’: English and Chinese, ‘word: index}, i.e, {‘english’: 1234}, {1234: ‘english’}.\n",
    "3. Sort the dictionaries to reduce padding.\n",
    "4. Split the dataset into patches for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdf0a81-7b85-4667-99cd-a93eb882a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    \"\"\"\n",
    "    Add padding to a batch of data\n",
    "    \"\"\"\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([\n",
    "            x, [padding] * (ML - len(x))\n",
    "        ]) if len(x) < ML else x for x in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725a6d19-70df-45ea-b746-625a22b546ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, train_file, dev_file):\n",
    "        # 1. Read the data and tokenize\n",
    "        self.train_en, self.train_cn = self.load_data(train_file)\n",
    "        self.dev_en, self.dev_cn = self.load_data(dev_file)\n",
    "\n",
    "        # 2. build dictionary: En and CN\n",
    "        self.en_word_dict, self.en_total_words, self.en_index_dict = self.build_dict(self.train_en)\n",
    "        self.cn_word_dict, self.cn_total_words, self.cn_index_dict = self.build_dict(self.train_cn)\n",
    "\n",
    "        # 3. word to id by dictionary\n",
    "        self.train_en, self.train_cn = self.wordToID(self.train_en, self.train_cn, \n",
    "                                                     self.en_word_dict, self.cn_word_dict)\n",
    "        self.dev_en, self.dev_cn = self.wordToID(self.dev_en, self.dev_cn, \n",
    "                                                 self.en_word_dict, self.cn_word_dict)\n",
    "\n",
    "        # 4. batch, padding, and masking\n",
    "        self.train_data = self.splitBatch(self.train_en, self.train_cn, BATCH_SIZE)\n",
    "        self.dev_data = self.splitBatch(self.dev_en, self.dev_cn, BATCH_SIZE)\n",
    "\n",
    "    # Utility functions\n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        read data, tokenize the seence and add start and end marks(bos, eos)\n",
    "        for example:\n",
    "        en = [\n",
    "            [\"BOS\", \"i\", \"love\", \"you\", \"EOS\"],\n",
    "            [\"BOS\", \"me\", \"too\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        cn = [\n",
    "            [\"BOS\", \"我\", \"爱\", \"你\", \"EOS\"],\n",
    "            [\"BOS\", \"我\", \"也\", ,\"是\", \"EOS\"],\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        en = []\n",
    "        cn = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                en.append([\"BOS\"] + word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "                cn.append([\"BOS\"] + word_tokenize(\" \".join([w for w in line[1]])) + [\"EOS\"])\n",
    "        return en, cn\n",
    "    \n",
    "    def build_dict(self, sentences, max_words = 50000):\n",
    "        \"\"\"\n",
    "        sentences: list of word list\n",
    "        build dictionary as {key(word): value(id)}\n",
    "        \"\"\"\n",
    "        word_count = Counter()\n",
    "        for setence in sentences:\n",
    "            for s in sentence:\n",
    "                word_count[s] += 1\n",
    "                \n",
    "        ls = word_count.most_common(max_words)\n",
    "        total_words = len(ls) + 2 # BOS + EOS = 2\n",
    "        word_dict = {w[0]: index + 2  for index, w in enumerate(ls)}\n",
    "        word_dict[\"UNK\"] = UNK\n",
    "        word_dict[\"PAD\"] = PAD\n",
    "        # inverted index:  {key(id): value(word)}\n",
    "        index_dict = {v: k for k, v in word_dict.items()}\n",
    "        return word_dict, total_words, index_dict\n",
    "        \n",
    "        \n",
    "    def wordToID(self, en, cn, en_dict, cn_dict, sort=True):\n",
    "        \"\"\"\n",
    "        convert input/output word lists to id lists\n",
    "        use input word list length to sort, reduce padding\n",
    "        \"\"\"\n",
    "        length = len(en)\n",
    "        out_en_ids = [[en_dict.get(w, 0) for w in sent] for sent in en]\n",
    "        out_cn_ids = [[cn_dict.get(w, 0) for w in sent] for sent in cn]\n",
    "        \n",
    "        def len_argsort(seq):\n",
    "            \"\"\"\n",
    "            get sorted index w.r.t length.\n",
    "            \"\"\"\n",
    "            \n",
    "            return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "        \n",
    "        if sort:\n",
    "            sorted_index = len_argsort(out_en_ids) # English\n",
    "            out_en_ids = [out_en_ids[id] for id in sorted_index]\n",
    "            out_cn_ids = [out_cn_ids[id] for id in sorted_index]\n",
    "            \n",
    "        return out_en_ids, out_cn_Ids\n",
    "    \n",
    "    def splitBatch(self, en, cn, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        get data into batches\n",
    "        \"\"\"\n",
    "        idx_list = np.arange(0, len(en), batch_size) # start, stop, step\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx_list)\n",
    "\n",
    "        batch_indexs = []\n",
    "        for idx in idx_list:\n",
    "            # batch index between current index and the min index o\n",
    "            batch_indexs.append(np.arange(idx, min(idx+batch_size, len(en)))) \n",
    "\n",
    "        batches = []\n",
    "        for batch_index in batch_indexs:\n",
    "            batch_en = [en[index] for index in batch_index]\n",
    "            batch_cn = [cn[index] for index in batch_index]\n",
    "            # paddings: batch, batch_size, batch_maxlen\n",
    "            batch_cn = seq_padding(batch_cn)\n",
    "            batch_en = seq_padding(batch_en)\n",
    "            # Batch class will be defined later which is the masking batch of data during training\n",
    "            # \"Object for holding a batch of data with mask during training.\"\n",
    "            batches.append(Batch(batch_en, batch_cn)) \n",
    "            \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa805da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Batch:\n",
    "#     \"Object for holding a batch of data with mask during training.\"\n",
    "#     def __init__(self, src, trg=None, pad=0):\n",
    "#         # convert words id to long format.  \n",
    "#         src = torch.from_numpy(src).to(DEVICE).long()\n",
    "#         trg = torch.from_numpy(trg).to(DEVICE).long()\n",
    "#         self.src = src\n",
    "#         # get the padding postion binary mask\n",
    "#         # change the matrix shape to  1×seq.length\n",
    "#         self.src_mask = (src != pad).unsqueeze(-2)\n",
    "#         # 如果输出目标不为空，则需要对decoder要使用到的target句子进行mask\n",
    "#         if trg is not None:\n",
    "#             # decoder input from target \n",
    "#             self.trg = trg[:, :-1]\n",
    "#             # decoder target from trg \n",
    "#             self.trg_y = trg[:, 1:]\n",
    "#             # add attention mask to decoder input  \n",
    "#             self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "#             # check decoder output padding number\n",
    "#             self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "#     # Mask \n",
    "#     @staticmethod\n",
    "#     def make_std_mask(tgt, pad):\n",
    "#         \"Create a mask to hide padding and future words.\"\n",
    "#         tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "#         tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "#         return tgt_mask # subsequent_mask is defined in 'decoder' section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edccc0-f34b-487b-b121-24a80ac9d526",
   "metadata": {},
   "source": [
    "Understanding Initialization:\n",
    "\n",
    "When you create an nn.Embedding layer, it initializes a lookup table with random embedding vectors for each word in the vocabulary.\n",
    "These initial vectors have a specific dimensionality (d_model) but their values are randomly chosen within a certain range.\n",
    "Normalization and Gradient Vanishing:\n",
    "\n",
    "Without the math.sqrt(d_model) factor, the initial values of the embedding vectors can have a large magnitude (very high or very low values).\n",
    "This can lead to two potential issues:\n",
    "Normalization: If the initial values have a large magnitude, the gradients during training might become very small when backpropagated through the network. This is known as the vanishing gradient problem, which can hinder the learning process.\n",
    "Activation Functions: If the network uses activation functions with bounded outputs (like sigmoid or tanh), large initial values can cause these activations to saturate, effectively making them insensitive to further changes.\n",
    "The Role of math.sqrt(d_model):\n",
    "\n",
    "Multiplying the embedding vectors by math.sqrt(d_model) essentially scales their initial values. This scaling helps address the issues mentioned above:\n",
    "Normalization: By dividing the variance of the initial values by d_model, the gradients tend to have a more manageable magnitude during backpropagation, improving learning efficiency.\n",
    "Activation Functions: Scaling the initial values ensures they are within a range where activation functions can operate effectively, allowing for more nuanced gradients during training.\n",
    "Alternative Initializations:\n",
    "\n",
    "While math.sqrt(d_model) is a common scaling factor, it's not the only approach. Some researchers use other techniques like uniform initialization within a specific range or initialization based on pre-trained word embeddings from sources like Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5975e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output embeddngs\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # lut -> lookup table\n",
    "        self.lut = nn.Embeddings(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return x's embedding vector (times math.sqrt(d_model))\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc956b",
   "metadata": {},
   "source": [
    "# Positional encoding\n",
    "[max_sequence_len, embedding_dim] \n",
    "$$PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "$$PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{2i/d_{model}}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b157f",
   "metadata": {},
   "source": [
    "**1. Standard Formula and Scaling Factor:**\n",
    "\n",
    "The standard formula for positional encoding in Transformers defines a scaling factor that influences the influence of position on the final embedding:\n",
    "\n",
    "```\n",
    "PE(pos, 2i) = sin(pos * 10000.0^(2i / d_model))\n",
    "PE(pos, 2i + 1) = cos(pos * 10000.0^(2i / d_model))\n",
    "```\n",
    "\n",
    "Here, the key term is `10000.0^(2i / d_model)`. Raising 10000.0 to a power that decreases with position (`i`) (due to the division by `d_model`) creates a scaling effect:\n",
    "\n",
    "- For positions closer to zero (smaller `i`), the value is closer to 10000.0, giving the position a stronger influence.\n",
    "- As the position (`i`) increases, the power term gets smaller, reducing the influence of position on the sine or cosine function.\n",
    "\n",
    "**2. Code and Its Scaling Effect:**\n",
    "\n",
    "The code snippet calculates a component for the positional encoding, focusing on even positions (`i`):\n",
    "\n",
    "```\n",
    "torch.exp(torch.arange(0., d_model, 2, device=DEVICE) * -(math.log(10000.0) / d_model))\n",
    "```\n",
    "\n",
    "This code achieves a similar scaling effect as the standard formula, but using logarithms:\n",
    "\n",
    "- `torch.exp(...)`: Applies the exponential function (e raised to the power of...).\n",
    "- `torch.arange(0., d_model, 2, device=DEVICE)`: Creates a sequence of increasing values for even positions.\n",
    "- `- (math.log(10000.0) / d_model)`: A constant term calculated as the negative logarithm of 10000.0 divided by `d_model`. The negative sign ensures the base of the exponential term is less than 1, creating a decaying sequence.\n",
    "\n",
    "**3. The Connection:**\n",
    "\n",
    "Here's why the code achieves a similar effect:\n",
    "\n",
    "- **Decaying Sequence:** The exponential term (`torch.exp(...)`) with the negative scaling factor creates a sequence of values that decay as the position (`i`) increases in the `torch.arange` part. This mimics the decreasing power term in the standard formula.\n",
    "- **Scaling Factor and Logarithm:** The constant term `- (math.log(10000.0) / d_model)` plays a crucial role. Let's analyze it:\n",
    "  - `math.log(10000.0)`: This calculates the natural logarithm (base e) of 10000.0. A larger value like 10000.0 in the logarithm typically results in a value close to 4 (logarithm of 10000 to base e is approximately 4.6).\n",
    "  - `- (...)`: The negative sign flips the result, ensuring the base of the exponential term in `torch.exp` is less than 1.\n",
    "  - `/ d_model`: This division scales the effect based on the model's dimension (`d_model`).\n",
    "\n",
    "**Essentially, raising `e` (base of the natural logarithm) to the power of `- (math.log(10000.0) / d_model)` results in a value close to 1/(10000.0^1), scaled by `d_model`. This acts similarly to the inverse of the scaling factor in the standard formula.**\n",
    "\n",
    "**In simpler terms:**\n",
    "\n",
    "- The standard formula uses `10000.0` raised to a power that decreases with position (`i`) to create a scaling effect.\n",
    "- The code achieves a similar effect by taking the inverse logarithm of 10000.0, dividing by `d_model`, and using that as the base of a decaying exponential term. The final result is mathematically equivalent (or very close) to the inverse of the scaling factor in the standard formula.\n",
    "\n",
    "**Deriving the Formula:**\n",
    "\n",
    "While the code doesn't directly calculate the inverse of the standard formula's scaling factor, it achieves a functionally equivalent outcome. Here's a breakdown of the steps involved:\n",
    "\n",
    "1. Standard Formula Scaling Factor:\n",
    "   - `10000.0^(2i / d_model)` (this term controls the influence of position in the standard formula).\n",
    "2. Code's Scaling Effect:\n",
    "   - The negative logarithm term (`-math.log(10000.0)`) in the code roughly scales the values down to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e1da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model, device=DEVICE)\n",
    "        position=torch.arange(0., max_len, device=DEVICE).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2, \n",
    "                                          device=DEVICE) * -(math.log(10000.0) / d_model))\n",
    "        pe_pos = torch.mul(position, div_term)\n",
    "        pe[:, 0::2] = torch.sin(pe_pos)  # even embedding dimension\n",
    "        pe[:, 1::2] = torch.cos(pe_pos)  # odd  embedding dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)  # pe\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fae3e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.zeros(50, 50, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cee5c0d-5635-451e-8c35-a1484f7497a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2e9c40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 25])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:, 0::2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efd36bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:, 1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4931afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
