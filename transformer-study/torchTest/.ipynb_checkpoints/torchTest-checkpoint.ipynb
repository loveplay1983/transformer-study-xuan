{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e2f785-6407-4a9a-aa86-490f1d60d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f5d11f-2199-4b5b-a9c9-cdd0c7429438",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL+FILENAME).content\n",
    "    (PATH/FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3547dea4-be51-498a-95f3-dabdd860e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH/FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1744cd-4d44-4777-8c5c-58ac6a09f15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed95d082-c34a-4ae2-bd72-57cb6507f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individiaul data is stored as a flattened row of length 784 (28x28)\n",
    "# Show image data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becc868e-97fb-461c-b8e2-7205aa0443ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    plt.show()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739c0e17-b53a-4dbe-8fc0-a9076363db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce9ebc-1217-40d7-94cb-ebc74dbd7d81",
   "metadata": {},
   "source": [
    "# Neural net from scratch (no torch.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf3563a-248a-46e3-88d6-379132508d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# initializing the weights here with Xavier initialisation (by multiplying with 1/sqrt(n)).\n",
    "# xavier vs he initialization\n",
    "# https://stackoverflow.com/questions/48641192/xavier-and-he-normal-initialization-difference\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e26b3",
   "metadata": {},
   "source": [
    "Here's how we can derive the log-softmax formula (subtraction method) from the conventional `log(softmax(x))` approach and prove their equivalence:\n",
    "\n",
    "**Conventional Method (log(softmax(x))):**\n",
    "\n",
    "1. **Softmax:** Calculate the softmax of the input tensor `x`. \n",
    "   ```\n",
    "   softmax(x_i) = exp(x_i) / Î£ (exp(x_j)) for all j = 1 to K\n",
    "   ```\n",
    "2. **Logarithm:** Take the natural logarithm (base e) of each element in the softmax output.\n",
    "\n",
    "**Log-Softmax Formula (Subtraction Method):**\n",
    "\n",
    "```python\n",
    "def log_softmax(x):\n",
    "  return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "```\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "1. **Start with Softmax:** We can rewrite the softmax formula for all elements in the tensor `x` as:\n",
    "   ```\n",
    "   p(i) = exp(x_i) / Î£ (exp(x_j)) for all j = 1 to K (where p(i) represents the probability for class i)\n",
    "   ```\n",
    "\n",
    "2. **Take Logarithm of Both Sides:** Apply the natural logarithm (ln) to both sides of the equation:\n",
    "   ```\n",
    "   ln(p(i)) = ln(exp(x_i) / Î£ (exp(x_j)))\n",
    "   ```\n",
    "   ![](https://math.libretexts.org/@api/deki/files/110839/clipboard_ef429cbef3374b3d97ff237de8ddb7aaf.png?revision=1)\n",
    "   https://math.libretexts.org/Bookshelves/Calculus/Differential_Calculus_for_the_Life_Sciences_(Edelstein-Keshet)/10%3A_Exponential_Functions/10.03%3A_Inverse_Functions_and_Logarithms\n",
    "\n",
    "3. **Logarithm Properties:** Use the logarithmic property: ln(a / b) = ln(a) - ln(b):\n",
    "   ```\n",
    "   ln(p(i)) = ln(exp(x_i)) - ln(Î£ (exp(x_j)))\n",
    "   ```\n",
    "\n",
    "4. **Simplify:** Since ln(exp(x_i)) = x_i, the equation becomes:\n",
    "   ```\n",
    "   ln(p(i)) = x_i - ln(Î£ (exp(x_j)))\n",
    "   ```\n",
    "\n",
    "5. **Equivalent Form:** We want to express the equation without the explicit summation term (Î£(exp(x_j))). Notice that ln(Î£(exp(x_j))) is a constant value for all elements in a row of the tensor `x` (because it's the sum across the class dimension). We can represent this constant value as `C`:\n",
    "   ```\n",
    "   ln(p(i)) = x_i - C\n",
    "   ```\n",
    "\n",
    "**Connection to PyTorch code:**\n",
    "\n",
    "* The term `x.exp().sum(-1).log()` in the PyTorch code calculates this constant value `C` (the log of the summed exponentiated scores).\n",
    "* Subtracting `C` from `x_i` (represented by `x` in the code) achieves the same result as subtracting `ln(Î£(exp(x_j)))` from `ln(p(i))`.\n",
    "\n",
    "**Proof of Equivalence:**\n",
    "\n",
    "Exponentiating both sides of the equation `ln(p(i)) = x_i - C` gives us:\n",
    "\n",
    "```\n",
    "exp(ln(p(i))) = exp(x_i - C)\n",
    "```\n",
    "\n",
    "Using the property exp(ln(x)) = x, we get:\n",
    "\n",
    "```\n",
    "p(i) = exp(x_i) * exp(-C)\n",
    "```\n",
    "\n",
    "Since `C` is the same constant value for all elements in a row, we can bring it outside the summation for all classes `j`:\n",
    "\n",
    "```\n",
    "p(i) = exp(x_i) * (exp(-C) * Î£(exp(x_j)))\n",
    "```\n",
    "\n",
    "We know that the softmax definition makes the summation of exponentials across classes equal to 1:\n",
    "\n",
    "```\n",
    "Î£(exp(x_j)) = 1\n",
    "```\n",
    "\n",
    "Therefore:\n",
    "\n",
    "```\n",
    "p(i) = exp(x_i) * (exp(-C) * 1)\n",
    "```\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "```\n",
    "p(i) = exp(x_i - C)\n",
    "```\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "We derived the log-softmax formula (subtraction method) by taking the logarithm of both sides of the softmax equation and manipulating the terms. We showed that subtracting the constant term `C` (log of summed exponentials) from `x_i` is equivalent to taking the logarithm of the softmax in the conventional approach (`log(softmax(x))`). Both methods result in the log probabilities for each class, but the log-softmax formula offers better numerical stability during calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f59ad5c6-630d-482b-a85d-c59f7241b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "405fcb11-c385-45e2-b110-3f3970977610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.6988, -2.5133, -2.9418, -1.9297, -1.9954, -2.6864, -2.9602, -2.9723,\n",
       "         -2.4153, -1.9540], grad_fn=<SelectBackward0>),\n",
       " torch.Size([64, 10]),\n",
       " torch.Size([10]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape, preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df344313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    \"\"\"\n",
    "    negative log-likelihood\n",
    "    \"\"\"\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e81c360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1af4ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.6864, -2.1391, -1.8418, -2.2671, -2.6139, -2.7566, -1.9001, -1.7214,\n",
       "        -2.0989, -2.0677, -1.6828, -2.3155, -2.0593, -2.8179, -2.1852, -2.2520,\n",
       "        -2.4098, -2.0565, -2.4580, -2.5073, -1.8987, -2.2315, -2.5094, -2.1525,\n",
       "        -2.3023, -2.8604, -2.4624, -1.7369, -2.8714, -2.8260, -2.4856, -2.5921,\n",
       "        -2.7132, -2.6349, -2.0674, -2.4387, -2.6595, -2.1039, -2.2436, -2.7596,\n",
       "        -2.2961, -2.2113, -2.2848, -2.4796, -2.4501, -2.6681, -2.5750, -2.1797,\n",
       "        -2.1050, -1.5586, -1.8730, -2.2412, -2.0343, -2.0029, -2.2814, -2.5664,\n",
       "        -2.4903, -2.6057, -2.1268, -2.2998, -2.0354, -2.2569, -2.6062, -2.2716],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[range(64), y_train[0:bs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9729d6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a2872ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55ff1f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3107, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c5f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
