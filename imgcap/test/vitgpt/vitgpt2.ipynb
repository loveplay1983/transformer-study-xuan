{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f2fa2d-c31e-4621-86af-f16f049547c0",
   "metadata": {},
   "source": [
    "<!-- # 1. ViT Vision Transformer\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\n",
    "> The Vision Transformer, or ViT, is a model for image classification that employs a Transformer-like architecture over patches of the image. An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used.\n",
    "\n",
    "# 2. GPT2\n",
    "![](https://jalammar.github.io/images/gpt2/gpt-2-layers-2.png)\n",
    "> GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cdf9c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9ac6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# hugginface\n",
    "import datasets  # https://pypi.org/project/datasets/\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "#     ViTFeatureExtractor,\n",
    "    AutoTokenizer,\n",
    "    GPT2Config,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914388f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available\n",
      "The GPU(s) are as follows:\n",
      "0 : NVIDIA GeForce GTX 1080 Ti\n",
      "1 : NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"There are {torch.cuda.device_count()} GPU(s) available\")\n",
    "#     print(f\"We will use the GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"The GPU(s) are as follows:\")\n",
    "    for each in range(torch.cuda.device_count()):\n",
    "        print(f\"{each} : {torch.cuda.get_device_properties(each).name}\")\n",
    "else:\n",
    "    print(\"No GPU available, usig the CPU instead.\")\n",
    "    device = torch.device(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7972b",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9279a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "class config: \n",
    "    ENCODER = \"google/vit-base-patch16-224\"\n",
    "    # ENCODER = \"google/vit-base-patch16-224-in21k\"\n",
    "    DECODER = 'gpt2'\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VAL_BATCH_SIZE = 1\n",
    "    VAL_EPOCHS = 1\n",
    "    LR = 5e-5\n",
    "    SEED = 42\n",
    "    MAX_LEN = 128\n",
    "    SUMMARY_LEN = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    TRAIN_PCT = 0.95\n",
    "    NUM_WORKERS = mp.cpu_count() # number of logical CPU cores\n",
    "    EPOCHS = 3\n",
    "    IMG_SIZE = (224, 224)\n",
    "    LABEL_MASK = -100\n",
    "    TOP_K = 1000\n",
    "    TOP_P = 0.95 \n",
    "#     CAPTION = \"/media/loveplay1983/data/ML/imgcap/Flickr-8k/captions.txt\"\n",
    "#     IMAGE_DIR = \"/media/loveplay1983/data/ML/imgcap/Flickr-8k/Images\"\n",
    "    CAPTION = \"/media/dllab/AI-PhD-Study/test-dataset/imgcap/flickr8k/captions.txt\"\n",
    "    IMAGE_DIR = \"/media/dllab/AI-PhD-Study/test-dataset/imgcap/flickr8k/Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74e335-3048-468f-a9f1-8936856372a4",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098d7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    \"\"\"\n",
    "    bos - begin of special \n",
    "    eos - end of special \n",
    "    \"\"\"\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "AutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed61c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rouge = datasets.load_metric(\"rouge\", trust_remote_code=True)\n",
    "# rouge = evaluate.load(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     # all unnecessary tokens are removed\n",
    "#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "#     label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "#     rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "#     return {\n",
    "#         \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "#         \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "#         \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge = datasets.load_metric(\"rouge\", trust_remote_code=True)\n",
    "rouge = evaluate.load(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"]\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }\n",
    "\n",
    "evaluate.combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf90db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14abcf47",
   "metadata": {},
   "source": [
    "# Error with `'numpy.float64' object has no attribute 'mid'` \n",
    "\n",
    "- evaluate vs datasets  \n",
    "`Because evaluate module does not have mid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140077f",
   "metadata": {},
   "source": [
    "**Stemming**:\n",
    "\n",
    "- Stemming is a text processing technique that reduces words to their base or root form.\n",
    "- For example, \"running\", \"runs\", and \"ran\" would all be stemmed to \"run\".\n",
    "\n",
    "\n",
    "**Porter Stemmer**:\n",
    "\n",
    "- The Porter stemmer is a widely used algorithm for stemming English words.\n",
    "- It removes common suffixes like \"-ing\", \"-ed\", \"-s\", etc., aiming to capture the core meaning of the word.\n",
    "\n",
    "\n",
    "`use_stemmer` Option:\n",
    "\n",
    "- This option allows you to control whether the Porter stemmer is applied during text processing.\n",
    "- Setting it to True enables stemming, while setting it to False disables it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "441bf0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.5, recall=0.5, fmeasure=0.5)\n"
     ]
    }
   ],
   "source": [
    "test_rouge = datasets.load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    \n",
    "    return preds, labels\n",
    "\n",
    "preds = [\"hello world\", \"what is rouge\"]\n",
    "labels = [\"morning world\", \"what is rouge\"]\n",
    "\n",
    "\n",
    "preds,labels = postprocess_text(preds, labels)\n",
    "\n",
    "\n",
    "result = test_rouge.compute(predictions=preds,\n",
    "                            references=labels,\n",
    "                            rouge_types=[\"rouge2\"],\n",
    "                            use_stemmer=True)[\"rouge2\"].mid\n",
    "# result = {k: round(v*100, 4) for k, v in result.items()}\n",
    "# prediction_lens = [\n",
    "#     np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "# ]\n",
    "# result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d43fe-941b-48e9-a0b8-24be361f2203",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2111d6-5cad-4ad1-93ae-9270f8de378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor and tokenizer\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(config.ENCODER)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(config.ENCODER)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.DECODER)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08af2cfc-0e80-45e7-9328-86afcd98dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms and dataframe\n",
    "# img 224,224\n",
    "# normalization\n",
    "# converting img to tensor\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(config.IMG_SIZE), \n",
    "    transforms.ToTensor(),  # ToTensor convert the data into Tensor and constrain the data in [0,1]\n",
    "    transforms.Normalize(mean=0.0, std=1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a98bfa5-76c8-41a7-8404-03d42ffd9c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.CAPTION)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f728c04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64728"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sizeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "541673e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16182"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4961ae37-5e1d-44f9-a635-7b7ef4de0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6910df-a54c-444c-9a8b-7e55504889f0",
   "metadata": {},
   "source": [
    "The dataset is created following these steps: \n",
    "> - read the image using the Image function of PIL library\n",
    ">- The image is transformed using the transformed defined above\n",
    ">- The transformed image is passed through the feature extractor to extract the pixel values from the image\n",
    ">- The captions are loaded from the dataframe\n",
    ">- The captions are tokenized\n",
    ">- The tokenized captions are padded to max length\n",
    ">- The images and tokenized captions are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "918b7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def min_max_img(img):\n",
    "#     min = np.min(img)\n",
    "#     max = np.max(img)\n",
    "#     return (img - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d2fd69-ca9e-4b82-a5fb-d6bfa93182a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, tokenizer, feature_extractor, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = 50\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # choosing the img and caption name along column index\n",
    "        caption = self.df.caption.iloc[idx]\n",
    "        image = self.df.image.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir , image)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # using pytorch transform to process the image which loaded by PIL Image\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Generate image and caption embedding\n",
    "        # 1. using normalized data as input to feature_extractor for fast computation\n",
    "        # 2. converting the output of extracted feature back to pixel val by .pixel_values\n",
    "        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        captions = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length    \n",
    "        ).input_ids\n",
    "\n",
    "        # Filter captions\n",
    "        # this filtering step ensures that padding tokens within the captions are replaced \n",
    "        # with a specific value (here, -100) to prevent them from affecting the model's training.\n",
    "        captions = [\n",
    "            caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions    \n",
    "        ]\n",
    "\n",
    "        # Combine image and caption embedding into a dict \n",
    "        encoding = {\n",
    "            \"pixel_values\": pixel_values.squeeze(), \n",
    "            \"labels\": torch.tensor(captions)   \n",
    "        }\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e029d0c6-a40c-4224-b03b-dc2a3d409bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation dataset\n",
    "train_dataset = ImgDataset(\n",
    "    train_df, \n",
    "    root_dir = config.IMAGE_DIR,\n",
    "    tokenizer = tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "val_dataset = ImgDataset(\n",
    "    val_df,\n",
    "    root_dir=config.IMAGE_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44644da-b285-4259-b081-6cb50bf8f2ec",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964b9aec-9d52-4ee6-bc33-abd1965e785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(config.ENCODER, config.DECODER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eabd7ac",
   "metadata": {},
   "source": [
    "> The benefit of using the separation token ([SEP]) as an end-of-sequence (EOS) marker in text generation is indeed partially due to the fact that natural language sequences often don't end in long blocks of text, but rather in smaller chunks like paragraphs or sentences  \n",
    "\n",
    "\n",
    "**1. Setting Special Token IDs:**\n",
    "\n",
    "- **`# model.config.decoder_start_token_id = tokenizer.cls_token_id` (Commented Out):**\n",
    "  - This line, although commented out, attempts to set the decoder start token ID in the model's configuration. It uses the tokenizer's `cls_token_id` (classification token), which might not be ideal for text generation tasks.\n",
    "  - A more appropriate token for text generation is the `bos_token_id` (beginning-of-sequence) which is used later in the code (`model.config.decoder_start_token_id = tokenizer.bos_token_id`).\n",
    "- **`model.config.pad_token_id = tokenizer.pad_token_id`:**\n",
    "  - This line sets the pad token ID in the model's configuration, aligning it with the tokenizer's `pad_token_id`. Pad tokens are used for padding sequences to a fixed length during generation.\n",
    "\n",
    "**2. Verifying Vocabulary Size:**\n",
    "\n",
    "- **`# make sure vocab size is set correctly`** (Comment):\n",
    "  - This comment highlights the importance of ensuring that the vocabulary size (`model.config.vocab_size`) in the model's configuration matches the actual vocabulary size of the decoder (`model.config.decoder.vocab_size`). Any mismatch could lead to errors during generation.\n",
    "- **`model.config.vocab_size = model.config.decoder.vocab_size`:**\n",
    "  - This line explicitly sets the model's vocabulary size (`model.config.vocab_size`) to the decoder's vocabulary size (`model.config.decoder.vocab_size`). This ensures consistency and helps prevent potential issues.\n",
    "\n",
    "**3. Beam Search Parameters:**\n",
    "\n",
    "- **`model.config.eos_token_id = tokenizer.sep_token_id`:**\n",
    "  - This line sets the end-of-sequence (EOS) token ID in the model's configuration. It uses the tokenizer's `sep_token_id` (separation token) to mark the end of the generated sequence.\n",
    "- **`model.config.decoder_start_token_id = tokenizer.bos_token_id`:**\n",
    "  - This line correctly sets the decoder start token ID to the tokenizer's `bos_token_id`. This token signifies the beginning of the generated sequence.\n",
    "- **`model.config.max_length = 128`:**\n",
    "  - This line sets the maximum length of the generated sequence to 128 tokens. This limits the output length to avoid overly long or repetitive generations.\n",
    "- **`model.config.early_stopping = True`:**\n",
    "  - This line enables early stopping during beam search. The search stops after a certain number of beams are completed, potentially improving efficiency.\n",
    "- **`model.config.no_repeat_ngram_size = 3`:**\n",
    "  - This line sets the no-repeat n-gram size for beam search. It penalizes sequences that contain repeated n-grams (sequences of n consecutive tokens) of size 3 or less. This helps generate more diverse outputs.\n",
    "- **`model.config.length_penalty = 2.0`:**\n",
    "  - This line sets the length penalty for beam search. It favors shorter sequences by applying a penalty proportional to the sequence length. This discourages overly long outputs.\n",
    "- **`model.config.num_beams = 4`:**\n",
    "  - This line sets the number of beams to use in beam search. The model will explore and expand the 4 most promising partial sequences at each step. This allows for a balance between exploration and exploitation during generation.\n",
    "\n",
    "**In summary, these lines configure the model for text generation using beam search with specific parameters to control the length, diversity, and quality of the generated outputs.**\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "- The initial attempt to set `decoder_start_token_id` with `cls_token_id` is likely a mistake, and `bos_token_id` is more suitable.\n",
    "- Double-check that the tokenizer's special tokens (`bos_token_id`, `eos_token_id`, `pad_token_id`) align with the model's expectations.\n",
    "- You might need to adjust these parameters (e.g., `max_length`, `num_beams`) based on your specific task and desired output characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6dde78b-5295-4d5a-854d-2a673dda660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee97470-f6db-4a0e-b1d7-e7deed6556b1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8915b02a",
   "metadata": {},
   "source": [
    "> \"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the \"\n",
    "            \"--report_to flag to control the integrations used for logging result (for instance --report_to none).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2e4777-75c8-4e05-9717-7272c933fa56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='ViT_large_gpt2',\n",
    "    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.VAL_BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1024,  \n",
    "    save_steps=2048, \n",
    "    warmup_steps=1024,  \n",
    "    learning_rate = 5e-5,\n",
    "    #max_steps=1500, # delete for full training\n",
    "    num_train_epochs = config.EPOCHS, #TRAIN_EPOCHS\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=None   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c06c629d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2024' max='6069' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2024/6069 41:16 < 1:22:35, 0.82 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4046' max='4046' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4046/4046 2:35:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loveplay1983/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'mid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training with Seq2SeqTrainer \u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      4\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m feature_extractor,\n\u001b[1;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m default_data_collator,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1886\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1887\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1888\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1890\u001b[0m     )\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/trainer.py:2311\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2315\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2719\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   3573\u001b[0m     eval_dataloader,\n\u001b[1;32m   3574\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3575\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3578\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   3579\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   3580\u001b[0m )\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/torch/lib/python3.11/site-packages/transformers/trainer.py:3854\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3850\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3851\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3852\u001b[0m         )\n\u001b[1;32m   3853\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3854\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels))\n\u001b[1;32m   3855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3856\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     10\u001b[0m labels_ids[labels_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m     11\u001b[0m label_str \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(labels_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m rouge_output \u001b[38;5;241m=\u001b[39m rouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpred_str, references\u001b[38;5;241m=\u001b[39mlabel_str, rouge_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmid\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(rouge_output\u001b[38;5;241m.\u001b[39mprecision, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(rouge_output\u001b[38;5;241m.\u001b[39mrecall, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2_fmeasure\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(rouge_output\u001b[38;5;241m.\u001b[39mfmeasure, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     19\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'mid'"
     ]
    }
   ],
   "source": [
    "# Training with Seq2SeqTrainer \n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer = feature_extractor,\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83593531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2660156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
