{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f2fa2d-c31e-4621-86af-f16f049547c0",
   "metadata": {},
   "source": [
    "# 1. ViT Vision Transformer\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\n",
    "> The Vision Transformer, or ViT, is a model for image classification that employs a Transformer-like architecture over patches of the image. An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used.\n",
    "\n",
    "# 2. GPT2\n",
    "![](https://jalammar.github.io/images/gpt2/gpt-2-layers-2.png)\n",
    "> GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cdf9c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ac6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# hugginface\n",
    "import datasets  # https://pypi.org/project/datasets/\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "#     ViTFeatureExtractor,\n",
    "    AutoTokenizer,\n",
    "    GPT2Config,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914388f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available\n",
      "The GPU(s) are as follows:\n",
      "0 : NVIDIA GeForce RTX 3090\n",
      "1 : NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"There are {torch.cuda.device_count()} GPU(s) available\")\n",
    "#     print(f\"We will use the GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"The GPU(s) are as follows:\")\n",
    "    for each in range(torch.cuda.device_count()):\n",
    "        print(f\"{each} : {torch.cuda.get_device_properties(each).name}\")\n",
    "else:\n",
    "    print(\"No GPU available, usig the CPU instead.\")\n",
    "    device = torch.device(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7972b",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9279a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "class config: \n",
    "    ENCODER = \"google/vit-base-patch16-224\"\n",
    "    # ENCODER = \"google/vit-base-patch16-224-in21k\"\n",
    "    DECODER = 'gpt2'\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VAL_BATCH_SIZE = 1\n",
    "    VAL_EPOCHS = 1\n",
    "    LR = 5e-5\n",
    "    SEED = 42\n",
    "    MAX_LEN = 128\n",
    "    SUMMARY_LEN = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    TRAIN_PCT = 0.95\n",
    "    NUM_WORKERS = mp.cpu_count() # number of logical CPU cores\n",
    "    EPOCHS = 3\n",
    "    IMG_SIZE = (224, 224)\n",
    "    LABEL_MASK = -100\n",
    "    TOP_K = 1000\n",
    "    TOP_P = 0.95 \n",
    "    CAPTION = \"/media/loveplay1983/data/ML/imgcap/Flickr-8k/captions.txt\"\n",
    "    IMAGE_DIR = \"/media/loveplay1983/data/ML/imgcap/Flickr-8k/Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74e335-3048-468f-a9f1-8936856372a4",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098d7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    \"\"\"\n",
    "    bos - begin of special \n",
    "    eos - end of special \n",
    "    \"\"\"\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "AutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed61c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge = datasets.load_metric(\"rouge\", trust_remote_code=True)\n",
    "rouge = evaluate.load(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    # The tokenizer here is actually the feature extractor which is very likely a pretrained model\n",
    "    # The batch_decode methods is used to convert a batch of tokenized sequences back into human-redable txt\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str,\n",
    "                                rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_ouput.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d43fe-941b-48e9-a0b8-24be361f2203",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2111d6-5cad-4ad1-93ae-9270f8de378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor and tokenizer\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(config.ENCODER)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(config.ENCODER)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.DECODER)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08af2cfc-0e80-45e7-9328-86afcd98dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms and dataframe\n",
    "# img 224,224\n",
    "# normalization\n",
    "# converting img to tensor\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(config.IMG_SIZE), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.0, std=1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a98bfa5-76c8-41a7-8404-03d42ffd9c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.CAPTION)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4961ae37-5e1d-44f9-a635-7b7ef4de0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6910df-a54c-444c-9a8b-7e55504889f0",
   "metadata": {},
   "source": [
    "The dataset is created following these steps: \n",
    "> - read the image using the Image function of PIL library\n",
    ">- The image is transformed using the transformed defined above\n",
    ">- The transformed image is passed through the feature extractor to extract the pixel values from the image\n",
    ">- The captions are loaded from the dataframe\n",
    ">- The captions are tokenized\n",
    ">- The tokenized captions are padded to max length\n",
    ">- The images and tokenized captions are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "918b7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def min_max_img(img):\n",
    "#     min = np.min(img)\n",
    "#     max = np.max(img)\n",
    "#     return (img - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d2fd69-ca9e-4b82-a5fb-d6bfa93182a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, tokenizer, feature_extractor, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = 50\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # choosing the img and caption name along column index\n",
    "        caption = self.df.caption.iloc[idx]\n",
    "        image = self.df.image.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir , image)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # using pytorch transform to process the image which loaded by PIL Image\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Generate image and caption embedding\n",
    "        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        captions = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length    \n",
    "        ).input_ids\n",
    "\n",
    "        # Filter captions\n",
    "        # this filtering step ensures that padding tokens within the captions are replaced \n",
    "        # with a specific value (here, -100) to prevent them from affecting the model's training.\n",
    "        captions = [\n",
    "            caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions    \n",
    "        ]\n",
    "\n",
    "        # Combine image and caption embedding into a dict \n",
    "        encoding = {\n",
    "            \"pixel_values\": pixel_values.squeeze(), \n",
    "            \"labels\": torch.tensor(captions)   \n",
    "        }\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e029d0c6-a40c-4224-b03b-dc2a3d409bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation dataset\n",
    "train_dataset = ImgDataset(\n",
    "    train_df, \n",
    "    root_dir = config.IMAGE_DIR,\n",
    "    tokenizer = tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "val_dataset = ImgDataset(\n",
    "    val_df,\n",
    "    root_dir=config.IMAGE_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44644da-b285-4259-b081-6cb50bf8f2ec",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964b9aec-9d52-4ee6-bc33-abd1965e785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.11.ln_cross_attn.bias', 'h.10.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.9.ln_cross_attn.weight', 'h.3.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.2.ln_cross_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.7.ln_cross_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.8.ln_cross_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.bias', 'h.10.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.1.ln_cross_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.4.ln_cross_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.3.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.5.ln_cross_attn.bias', 'h.2.crossattention.c_proj.bias', 'h.0.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.7.ln_cross_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.6.ln_cross_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.6.crossattention.q_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.4.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.ln_cross_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.11.ln_cross_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(config.ENCODER, config.DECODER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eabd7ac",
   "metadata": {},
   "source": [
    "> The benefit of using the separation token ([SEP]) as an end-of-sequence (EOS) marker in text generation is indeed partially due to the fact that natural language sequences often don't end in long blocks of text, but rather in smaller chunks like paragraphs or sentences  \n",
    "\n",
    "\n",
    "**1. Setting Special Token IDs:**\n",
    "\n",
    "- **`# model.config.decoder_start_token_id = tokenizer.cls_token_id` (Commented Out):**\n",
    "  - This line, although commented out, attempts to set the decoder start token ID in the model's configuration. It uses the tokenizer's `cls_token_id` (classification token), which might not be ideal for text generation tasks.\n",
    "  - A more appropriate token for text generation is the `bos_token_id` (beginning-of-sequence) which is used later in the code (`model.config.decoder_start_token_id = tokenizer.bos_token_id`).\n",
    "- **`model.config.pad_token_id = tokenizer.pad_token_id`:**\n",
    "  - This line sets the pad token ID in the model's configuration, aligning it with the tokenizer's `pad_token_id`. Pad tokens are used for padding sequences to a fixed length during generation.\n",
    "\n",
    "**2. Verifying Vocabulary Size:**\n",
    "\n",
    "- **`# make sure vocab size is set correctly`** (Comment):\n",
    "  - This comment highlights the importance of ensuring that the vocabulary size (`model.config.vocab_size`) in the model's configuration matches the actual vocabulary size of the decoder (`model.config.decoder.vocab_size`). Any mismatch could lead to errors during generation.\n",
    "- **`model.config.vocab_size = model.config.decoder.vocab_size`:**\n",
    "  - This line explicitly sets the model's vocabulary size (`model.config.vocab_size`) to the decoder's vocabulary size (`model.config.decoder.vocab_size`). This ensures consistency and helps prevent potential issues.\n",
    "\n",
    "**3. Beam Search Parameters:**\n",
    "\n",
    "- **`model.config.eos_token_id = tokenizer.sep_token_id`:**\n",
    "  - This line sets the end-of-sequence (EOS) token ID in the model's configuration. It uses the tokenizer's `sep_token_id` (separation token) to mark the end of the generated sequence.\n",
    "- **`model.config.decoder_start_token_id = tokenizer.bos_token_id`:**\n",
    "  - This line correctly sets the decoder start token ID to the tokenizer's `bos_token_id`. This token signifies the beginning of the generated sequence.\n",
    "- **`model.config.max_length = 128`:**\n",
    "  - This line sets the maximum length of the generated sequence to 128 tokens. This limits the output length to avoid overly long or repetitive generations.\n",
    "- **`model.config.early_stopping = True`:**\n",
    "  - This line enables early stopping during beam search. The search stops after a certain number of beams are completed, potentially improving efficiency.\n",
    "- **`model.config.no_repeat_ngram_size = 3`:**\n",
    "  - This line sets the no-repeat n-gram size for beam search. It penalizes sequences that contain repeated n-grams (sequences of n consecutive tokens) of size 3 or less. This helps generate more diverse outputs.\n",
    "- **`model.config.length_penalty = 2.0`:**\n",
    "  - This line sets the length penalty for beam search. It favors shorter sequences by applying a penalty proportional to the sequence length. This discourages overly long outputs.\n",
    "- **`model.config.num_beams = 4`:**\n",
    "  - This line sets the number of beams to use in beam search. The model will explore and expand the 4 most promising partial sequences at each step. This allows for a balance between exploration and exploitation during generation.\n",
    "\n",
    "**In summary, these lines configure the model for text generation using beam search with specific parameters to control the length, diversity, and quality of the generated outputs.**\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "- The initial attempt to set `decoder_start_token_id` with `cls_token_id` is likely a mistake, and `bos_token_id` is more suitable.\n",
    "- Double-check that the tokenizer's special tokens (`bos_token_id`, `eos_token_id`, `pad_token_id`) align with the model's expectations.\n",
    "- You might need to adjust these parameters (e.g., `max_length`, `num_beams`) based on your specific task and desired output characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6dde78b-5295-4d5a-854d-2a673dda660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee97470-f6db-4a0e-b1d7-e7deed6556b1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8915b02a",
   "metadata": {},
   "source": [
    "> \"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the \"\n",
    "            \"--report_to flag to control the integrations used for logging result (for instance --report_to none).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2e4777-75c8-4e05-9717-7272c933fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='ViT_large_gpt2',\n",
    "    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.VAL_BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1024,  \n",
    "    save_steps=2048, \n",
    "    warmup_steps=1024,  \n",
    "    learning_rate = 5e-5,\n",
    "    #max_steps=1500, # delete for full training\n",
    "    num_train_epochs = config.EPOCHS, #TRAIN_EPOCHS\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=None   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c629d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loveplay1983/Workstation/Anaconda/anaconda/envs/torch/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/loveplay1983/Workstation/Anaconda/anaconda/envs/torch/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1204' max='6069' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1204/6069 11:02 < 44:41, 1.81 it/s, Epoch 0.59/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training with Seq2SeqTrainer \n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer = feature_extractor,\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83593531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2660156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
